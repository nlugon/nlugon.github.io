<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Photogrammetry Fundamentals - Part 1 | Noah Lugon-Moulin </title> <meta name="author" content="Noah Lugon-Moulin"> <meta name="description" content="Summary notes on Computer Vision, Vision-Based Localization and 3D Reconstruction"> <meta name="keywords" content="portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://nlugon.github.io/blog/2025/photogrammetry-roadmap/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Noah</span> Lugon-Moulin </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Posts </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Photogrammetry Fundamentals - Part 1</h1> <p class="post-meta"> Created in October 26, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/camera"> <i class="fa-solid fa-hashtag fa-sm"></i> camera</a>   <a href="/blog/tag/photogrammetry"> <i class="fa-solid fa-hashtag fa-sm"></i> photogrammetry</a>   <a href="/blog/tag/computer-vision"> <i class="fa-solid fa-hashtag fa-sm"></i> computer-vision</a>   <a href="/blog/tag/slam"> <i class="fa-solid fa-hashtag fa-sm"></i> slam</a>   <a href="/blog/tag/visual-odometry"> <i class="fa-solid fa-hashtag fa-sm"></i> visual-odometry</a>   ·   <a href="/blog/category/robotics"> <i class="fa-solid fa-tag fa-sm"></i> robotics</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/skydio-feature-tracking-480.webp 480w,/assets/img/skydio-feature-tracking-800.webp 800w,/assets/img/skydio-feature-tracking-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/skydio-feature-tracking.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Skydio Feature Tracking" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p class="text-muted text-center mt-2">Source: Skydio</p> <p>Cameras are an incredibly powerful and important sensor for robotics, as they provide a way to perceive the world in a very similar way we do, using our eyes. Wherever there is a light source, a camera provides a very rich and dense amount of information, from which can not only be recognized what different elements are inside of an environment (object recognition), but also from which can be determined your location inside of the environment (localization), and determined the environment’s 3D structured (3D reconstruction/mapping).</p> <p>Especially when dealing with sequences of images (i.e. videos/live camera feed), the amount of information to process becomes extremely large, and multiple methods and techniques should be known to process all of the data efficiently, particularly if this needs to be processed in real-time and on computation-limited setups. One great resource I came accross is Cyrill Stachniss’ Photogrammetry YouTube lecture series, which goes through all fundamentals of cameras and camera data processing, from image smoothing to visual SLAM. So here are some compact notes on the concepts he covers in each of his lectures, which provide an overview and refresher of all core topics.</p> <h2 id="on-this-page">On this page</h2> <ul> <li><a href="#chapter-1--introduction-to-photogrammetry">Chapter 1 — Introduction to Photogrammetry</a></li> <li><a href="#chapter-2--camera-basics--propagation-of-light">Chapter 2 — Camera Basics &amp; Propagation of Light</a></li> <li><a href="#chapter-3--image-histograms--simple-point-operators">Chapter 3 — Image Histograms &amp; Simple Point Operators</a></li> <li><a href="#chapter-4--histogram-equalization-noise-variance-equalization--binary-images">Chapter 4 — Histogram Equalization, Noise Variance Equalization &amp; Binary Images</a></li> <li><a href="#chapter-5--local-operators-geometric-transforms--cross-correlation">Chapter 5 — Local Operators, Geometric Transforms &amp; Cross Correlation</a></li> <li><a href="#chapter-6--visual-features-keypoints--descriptors">Chapter 6 — Visual Features: Keypoints &amp; Descriptors</a></li> <li><a href="#chapter-7--image-segmentation-with-mean-shift">Chapter 7 — Image Segmentation with Mean Shift</a></li> <li><a href="#chapter-8--introduction-to-classification">Chapter 8 — Introduction to Classification</a></li> <li><a href="#chapter-9--neural-networks">Chapter 9 — Neural Networks</a></li> <li><a href="#chapter-10--math-basics-&amp;-homogeneous-transforms">Chapter 10 — Math Basics &amp; Homogeneous Transforms</a></li> <li> <p><a href="#chapter-11--camera-parameters-calibration-and-P3P">Chapter 11 — Camera Parameters, Calibration, and P3P</a></p> </li> <li><a href="#references">References</a></li> </ul> <hr> <h1 id="chapter-1--introduction-to-photogrammetry">Chapter 1 — Introduction to Photogrammetry</h1> <p><strong>Why cameras?</strong> Contact-free, dense/cheap/fast capture; human-interpretable; supports dynamics &amp; real-time.<br> <strong>Limits.</strong> Needs light; measures directional intensity only; occlusions; loss of depth information with 3D to 2D projection; other sensors may be more precise.</p> <p><strong>Key idea.</strong> A camera measures light intensities accross a large array of pixels. Each pixel corresponds to a direction at which the light ray(s) entered the camera<br> <strong>Pipelines.</strong> <em>Forward:</em> object → physics → intrinsics/extrinsics → image. <em>Inverse:</em> images + models + priors → object geometry/semantics.<br> <strong>Algorithms are central.</strong> Sensors ≈ eyes; estimation ≈ brain → implement methods.</p> <p><strong>Sensors &amp; apps.</strong> Industrial/consumer/aerial cameras, LiDAR; mapping, orthophotos, city models, cultural heritage, robotics, autonomous driving.</p> <hr> <h1 id="chapter-2--camera-basics--propagation-of-light">Chapter 2 — Camera Basics &amp; Propagation of Light</h1> <p><strong>What is measured?</strong> Pixels integrate photons over exposure time → <strong>intensity</strong> per <strong>direction</strong> (ray).<br> <strong>Camera elements.</strong> Lens &amp; <strong>aperture</strong>, <strong>shutter</strong> (rolling/global), <strong>sensor</strong> (photosites), <strong>A/D</strong>, post-processing (incl. <strong>demosaicing</strong>).</p> <p><strong>Color imaging.</strong> Three-chip prism vs <strong>single-chip CFA</strong> (Bayer: 50% G); <strong>demosaicing</strong> adds artifacts (edges/moire).<br> <strong>Shutter.</strong> <strong>Rolling</strong> (row-wise timing skew) vs <strong>global</strong> (simultaneous). Prefer global for geometry/high-speed scenes.</p> <p><strong>Image formation.</strong> <strong>Pinhole</strong> (one ray per point → sharp but dark). <strong>Thin lens</strong> (inline: \(\tfrac{1}{f} = \tfrac{1}{z} + \tfrac{1}{z'}\)); aperture limits off-axis error.</p> <p><strong>Aperture &amp; DoF.</strong> Higher f-number → more DoF, less light; diffraction at extremes.</p> <p><strong>Lenses &amp; FOV.</strong> Tele (narrow, minimal perspective), normal, wide (perspective exaggeration), <strong>fisheye</strong> (curved lines).<br> <strong>Aberrations.</strong> <strong>Distortion</strong> (barrel/pincushion/mustache), <strong>spherical</strong>, <strong>chromatic</strong>, <strong>astigmatism</strong>, <strong>coma</strong>, <strong>vignetting</strong>. Mitigate via optics, aperture, calibration.</p> <p><strong>Light models.</strong> <strong>Ray</strong> (Snell/Fresnel, reversibility), <strong>wave</strong> (Maxwell; interference/diffraction; 400–700 nm), <strong>particle</strong> (photons).<br> <strong>Exposure triangle.</strong> Shutter ↔ motion blur; Aperture ↔ DoF; ISO ↔ noise/gain.</p> <p><em>Suggested figures:</em> pinhole geometry; rolling vs global; exposure triangle.</p> <hr> <h1 id="chapter-3--image-histograms--simple-point-operators">Chapter 3 — Image Histograms &amp; Simple Point Operators</h1> <p><strong>Image &amp; histogram.</strong> Grayscale image \(I\) is an \(M\times N\) matrix; intensities in \([0, 255]\).<br> <strong>Histogram</strong> \(h[g]\): count of pixels with intensity \(g\). <strong>CDF</strong> \(H[g]\): cumulative counts/probabilities.</p> <p><strong>What histograms tell.</strong> Brightness (mean), contrast (variance), robustness (median). Shape hints at lighting/contrast, saturation/clipping.<br> <strong>Compute.</strong> One pass over pixels → \(\mathcal{O}(MN)\). Color images: per-channel histograms.</p> <p><strong>Operators (by support).</strong> Global, <strong>point</strong>, local.<br> <strong>Point operators.</strong> Map \(g \mapsto f(g)\) independently per pixel (fast via LUT of 256 entries).</p> <p><strong>Linear transform.</strong> \(g' = a\,g + b\).</p> <ul> <li>Adjust brightness (\(b\)) and contrast (\(a\)).</li> <li> <table> <tbody> <tr> <td>Propagation of mean/std: \(\mu' = a\mu + b\), $$ \sigma’ =</td> <td>a</td> <td>\sigma $$.</td> </tr> </tbody> </table> </li> <li>Contrast inversion: \(g' = 255 - g\).</li> </ul> <p><strong>Nonlinear examples.</strong> <strong>Thresholding</strong> → binary image; <strong>quantization</strong> → map to \(K\) discrete levels (for \(K = 2\) gives binary).</p> <p><em>Suggested figures:</em> sample image &amp; histogram; thresholding example; tone curve.</p> <hr> <h1 id="chapter-4--histogram-equalization-noise-variance-equalization--binary-images">Chapter 4 — Histogram Equalization, Noise Variance Equalization &amp; Binary Images</h1> <h2 id="41-histogram-equalization-he">4.1 Histogram Equalization (HE)</h2> <p><strong>Goal.</strong> Redistribute intensities to <strong>use full dynamic range</strong> and enhance contrast.<br> <strong>Monotone mapping and PDFs.</strong> For monotone \(b = f(a)\), conservation of area yields</p> \[h_b(b) = h_a(a)\,\left|\frac{da}{db}\right|, \quad a = f^{-1}(b).\] <p><strong>HE solution.</strong> Use the <strong>CDF</strong> of input: \(b = \alpha\,H(a) + \beta\) → approximately uniform output (discrete case not perfectly flat).<br> <strong>Effect.</strong> Boosts contrast, especially in low-contrast regions; may amplify noise. Variants: <strong>AHE</strong>, <strong>CLAHE</strong> (limits amplification).</p> <p><em>Suggested figures:</em> before/after image &amp; histograms; CDF mapping sketch.</p> <h2 id="42-noise-variance-equalization-nve">4.2 Noise Variance Equalization (NVE)</h2> <p><strong>Photon noise.</strong> Counts follow <strong>Poisson</strong> → mean = variance \(\propto\) intensity; plus constant read noise.<br> <strong>Goal.</strong> Make variance <strong>independent of intensity</strong>. For variance \(\sigma_g^2 \propto g\) and monotone \(g' = f(g)\), choose \(f\) so that \(\sigma_{g'}^2\) is constant.<br> <strong>Derivation (sketch).</strong> Variance propagation gives \(f'(g) \propto \tfrac{1}{\sqrt{g}}\), hence</p> \[g' = c\,\sqrt{g} + d\] <p>(<strong>Anscombe-like</strong> transform). Dark regions are stretched; bright compressed.</p> <p><em>Suggested figure:</em> effect of \(\sqrt{\cdot}\) mapping on a gradient.</p> <h2 id="43-binary-images--common-operations">4.3 Binary Images &amp; Common Operations</h2> <p><strong>Binary images.</strong> Pixels in {0,1}. Often obtained via thresholding or segmentation masks.</p> <h3 id="connected-components-cc">Connected components (CC)</h3> <ul> <li> <strong>Neighborhoods:</strong> <strong>N4</strong> (up/down/left/right) vs <strong>N8</strong> (plus diagonals).</li> <li> <strong>Labeling (grid exploit):</strong> single pass creates provisional labels using processed neighbors; equivalence table resolves merges; second pass compacts labels.</li> <li> <strong>Complexity:</strong> linear in the number of foreground pixels.</li> </ul> <h3 id="distance-transform-dt">Distance transform (DT)</h3> <ul> <li> <strong>Task:</strong> distance of each pixel to nearest border.</li> <li> <strong>Two-pass scheme:</strong> TL→BR then BR→TL, keep min; versions for <strong>N4</strong> and <strong>N8</strong> (under/over-estimate Euclidean).</li> <li> <strong>Better approx:</strong> combine straight + diagonal costs; <strong>EDT</strong> (exact) available in libraries.</li> </ul> <h3 id="morphology">Morphology</h3> <ul> <li> <strong>Erosion:</strong> shrinks foreground; removes outliers.</li> <li> <strong>Dilation:</strong> expands foreground; fills holes.</li> <li> <strong>Opening = erosion → dilation:</strong> removes small specks.</li> <li> <strong>Closing = dilation → erosion:</strong> fills small holes.</li> <li>Useful to <strong>clean masks</strong> before CC/DT.</li> </ul> <p><em>Suggested figures:</em> CC labeling example; N4 vs N8 DT; opening/closing before-after.</p> <hr> <h1 id="chapter-5--local-operators-geometric-transforms--cross-correlation">Chapter 5 — Local Operators, Geometric Transforms &amp; Cross Correlation</h1> <blockquote> <p>Local (neighborhood) operators via <strong>convolution</strong>, <strong>geometric image warps + resampling</strong>, and <strong>template matching</strong> with <strong>normalized cross correlation (NCC)</strong>.</p> </blockquote> <h2 id="51-local-operators-via-convolution--smoothing--gradients">5.1 Local Operators via Convolution — Smoothing &amp; Gradients</h2> <p><strong>Operator types.</strong> Point (per‑pixel), <strong>local</strong> (neighborhood), global. Point ops struggle with noise/local structure; <strong>local</strong> ops aggregate neighbors.</p> <p><strong>Convolution.</strong> Discrete 2D: inline: \((f * k)[i,j] = \sum_u \sum_v f[i-u,\,j-v]\,k[u,v]\). Properties: <strong>commutative</strong>, <strong>associative</strong>, <strong>distributive</strong>.<br> <strong>Separable kernels.</strong> \(k(x,y)=k_x(x)\,k_y(y)\) → two 1D passes (faster).</p> <p><strong>Box filter.</strong> Mean over a window (kernel sums to <strong>1</strong>); reduces noise; border handling: constant, wrap, clamp, mirror.<br> <strong>Median filter.</strong> Robust to outliers (nonlinear).</p> <p><strong>Binomial/Gaussian smoothing.</strong> Binomial weights (Pascal triangle) ≈ discrete Gaussian; gentler smoothing than box for same window.<br> <strong>Integral image.</strong> Prefix sums for O(1) box sums per window.</p> <p><strong>Gradients (1st derivatives).</strong> Weighted differences with zero‑sum kernels; in 2D: \(\nabla I = (I_x, I_y)\), magnitude \(|\nabla I|\), direction \(\theta\).<br> <strong>Sobel (3×3)</strong> = smoothing × derivative; <strong>Scharr</strong> improves gradient <strong>direction</strong> accuracy.<br> <strong>2nd derivatives.</strong> 1D second difference \(f'' \approx [1, -2, 1]\); <strong>Laplacian</strong> \(\nabla^2 I = I_{xx}+I_{yy}\) for edges.</p> <p><em>Suggested figures:</em> kernel examples (box/binomial), Sobel masks, gradient magnitude image.</p> <h2 id="52-geometric-transformations--resampling">5.2 Geometric Transformations &amp; Resampling</h2> <p><strong>Transforms.</strong> Map output coords \(x'\) to input \(x\) via <strong>inverse warping</strong>: \(x = T^{-1}(x')\). Examples: translation, scale, rotation, affine; rectification, registration, texture mapping.</p> <p><strong>Interpolation.</strong> Assign intensity at non‑integer \(x\):</p> <ul> <li> <strong>Nearest neighbor</strong> (fast, blocky), <strong>bilinear</strong> (balanced), <strong>bicubic</strong> (smooth, costlier).</li> <li> <strong>Quality vs speed:</strong> NN (++) / (−); Bilinear (+) / (0); Bicubic (−) / (++).</li> </ul> <p><strong>Forward vs inverse warping.</strong> Forward leaves <strong>holes</strong>; <strong>prefer inverse</strong> then interpolate.</p> <p><strong>Subsampling (downscale).</strong> Naive decimation → <strong>aliasing</strong>. Pre‑filter with <strong>binomial/Gaussian</strong> before taking every nth pixel.<br> <strong>Kernel width &amp; scale.</strong> Smoothing strength depends on kernel std and local scale \(m\) of transform.<br> <strong>Image pyramids.</strong> Successively downsampled images (×1/2 per level) for multiscale processing.</p> <p><em>Suggested figures:</em> bilinear vs bicubic; forward vs inverse warping; pyramid illustration.</p> <h2 id="53-template-matching-with-normalized-cross-correlation-ncc">5.3 Template Matching with Normalized Cross Correlation (NCC)</h2> <p><strong>Goal.</strong> Find template \(g_2\) in image \(g_1\) under <strong>translation</strong>, allowing brightness/contrast changes.<br> <strong>NCC</strong> at offset \(\Delta\) (over overlap region):</p> \[\rho(\Delta) = \frac{\sum (g_1 - \mu_1)(g_2 - \mu_2)}{\sigma_1\,\sigma_2},\quad \rho\in[-1,1].\] <p><strong>Search.</strong> Exhaustive over \((\Delta x,\Delta y)\) or <strong>coarse‑to‑fine</strong> with an <strong>image pyramid</strong>.<br> <strong>Limitations.</strong> Assumes translation only, uniform noise, no occlusion; quality degrades with rotation (\gtrsim 20^\circ) or scale (\gtrsim 30\%).</p> <p><strong>Subpixel refinement.</strong> Fit a <strong>quadratic</strong> around the NCC peak; use gradient/Hessian to estimate argmax → ~0.1 px precision.</p> <p><em>Suggested figures:</em> NCC heatmap with peak; pyramid search schematic.</p> <hr> <h1 id="chapter-6--visual-features-keypoints--descriptors">Chapter 6 — Visual Features: Keypoints &amp; Descriptors</h1> <blockquote> <p>Find <strong>distinct points</strong> (keypoints) and describe their <strong>local appearance</strong> for matching across images.</p> </blockquote> <h2 id="61-keypoints-corners--blobs">6.1 Keypoints (Corners &amp; Blobs)</h2> <p><strong>Idea.</strong> Corners/texture‑rich spots are stable under translation/rotation/illumination and good for matching.</p> <p><strong>Structure matrix (second‑moment).</strong> From gradients \(I_x, I_y\) (e.g., Sobel/Scharr), build</p> <p>\begin{equation} M = \sum_{(u,v)\in\mathcal{N}} \begin{bmatrix} I_x^2 &amp; I_x I_y <br> I_x I_y &amp; I_y^2 \end{bmatrix}. \end{equation}</p> <ul> <li> <strong>Harris:</strong> \(R = \det(M) - k\,[\mathrm{trace}(M)]^2\) — large \(R\) ⇒ corner.</li> <li> <strong>Shi–Tomasi:</strong> threshold the <strong>smallest eigenvalue</strong> \(\lambda_{\min}\).</li> <li> <strong>Förstner:</strong> criteria on size/roundness of error ellipse; supports <strong>subpixel</strong> refinement.</li> </ul> <p><strong>Practical steps.</strong> Gray → smooth → gradients → <strong>corner response</strong> → <strong>threshold</strong> → <strong>non‑maximum suppression</strong>.</p> <p><strong>Difference of Gaussians (DoG).</strong> Build a scale‑space pyramid; per level blur with \(\sigma\), subtract consecutive blurs, find <strong>extrema across scales</strong> (corners/blobs). Suppress edge responses via eigenvalue‑ratio test.</p> <p><em>Suggested figure:</em> Harris/Shi–Tomasi responses with NMS (Figure X).</p> <h2 id="62-descriptors-sift-brief-orb">6.2 Descriptors (SIFT, BRIEF, ORB)</h2> <p><strong>Goal.</strong> Represent a keypoint’s neighborhood with a vector <strong>robust</strong> to viewpoint/illumination.</p> <p><strong>SIFT.</strong></p> <ul> <li>Keypoint: DoG extremum with <strong>scale</strong> and <strong>orientation</strong>.</li> <li>Descriptor: <strong>16×16</strong> window at keypoint scale → <strong>4×4</strong> cells × <strong>8‑orient</strong> hist = <strong>128D</strong> (normalized).</li> <li>Invariance: translation/rotation/scale; partial to lighting and affine.</li> <li>Matching: Euclidean distance + <strong>Lowe’s ratio test</strong>.</li> </ul> <p><strong>Binary descriptors.</strong> Fast &amp; compact.</p> <ul> <li> <strong>BRIEF:</strong> compare <strong>intensity pairs</strong> in a smoothed patch → bitstring; compare via <strong>Hamming distance</strong>.</li> <li> <strong>ORB:</strong> BRIEF + <strong>orientation</strong> (via moments) + <strong>learned pair selection</strong>; rotation‑invariant in‑plane, near real‑time.</li> </ul> <p><strong>Notes.</strong> Keep <strong>consistent pair set &amp; order</strong> for binary descriptors. For scale‑invariance with ORB, use an <strong>image pyramid</strong>.</p> <p><em>Suggested figures:</em> SIFT cell grid and histogram; ORB rotation compensation (Figure X).</p> <hr> <h1 id="chapter-7--image-segmentation-with-mean-shift">Chapter 7 — Image Segmentation with Mean Shift</h1> <blockquote> <p><strong>Unsupervised</strong> clustering of pixels in a <strong>joint feature space</strong> (e.g., color + position) by following <strong>density modes</strong>.</p> </blockquote> <p><strong>Kernel Density Estimation (KDE).</strong> With kernel \(K\) (often Gaussian) and bandwidths \(h_f, h_s\): estimate local density and compute the <strong>mean shift vector</strong> (center‑of‑mass shift).</p> <p><strong>Algorithm.</strong> 1) Choose kernel &amp; bandwidth(s).<br> 2) For each pixel feature vector, iteratively <strong>shift window to the local mean</strong> until convergence.<br> 3) <strong>Merge</strong> windows that converge within thresholds → <strong>regions</strong> (attraction basins).</p> <p><strong>Design.</strong></p> <ul> <li>Feature space: color (e.g., Lab/RGB), optionally <strong>spatial coords</strong>; bandwidths <strong>\(K_f\)</strong> (feature) and <strong>\(K_s\)</strong> (spatial) control granularity.</li> <li>Pros: no preset number of clusters; flexible shapes; robust to outliers.</li> <li>Cons: bandwidth selection matters; scales poorly with <strong>high‑dimensional</strong> features.</li> </ul> <p><strong>Tips.</strong> Use <strong>pyramids</strong> or subsampling for speed; post‑process with <strong>morphology</strong> and <strong>connected components</strong>.</p> <p><em>Suggested figure:</em> mean shift trajectories toward modes; segmentation examples (Figure X).</p> <hr> <h1 id="chapter-8--introduction-to-classification">Chapter 8 — Introduction to Classification</h1> <h2 id="81-motivation--problem-setup">8.1 Motivation &amp; Problem Setup</h2> <p><strong>Goal.</strong> Given features \(x\) and a finite set of classes \(\mathcal{C}\), learn a mapping \(f: \mathbb{R}^d \rightarrow \mathcal{C}\).<br> <strong>Examples.</strong> Vegetation/non‑vegetation from pixel spectra; “family car” from price/power.</p> <ul> <li> <strong>Supervised learning:</strong> labeled pairs \((x_i, y_i)\).</li> <li> <strong>Classification vs. regression:</strong> discrete labels vs. continuous outputs.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Two toy classes in 2‑D feature space with a separating boundary.</li> <li>Table contrasting classification vs. regression (outputs, losses, metrics).</li> </ul> <h2 id="82-hypotheses-version-space-and-margin">8.2 Hypotheses, Version Space, and Margin</h2> <ul> <li> <strong>Multiple consistent hypotheses:</strong> same training data, different separators.</li> <li> <strong>Version space:</strong> set of hypotheses consistent with the labels.</li> <li> <strong>Large margin principle:</strong> prefer separator maximizing distance to support points.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Most‑general vs. most‑specific hypothesis cones; shaded version space.</li> <li>Max‑margin separator with support vectors highlighted.</li> </ul> <h2 id="83-errors-confusion-matrix-and-metrics">8.3 Errors, Confusion Matrix, and Metrics</h2> <p>Let TP, FP, TN, FN denote counts.</p> <ul> <li> <strong>Precision:</strong> \(\text{Prec}=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FP}}\)</li> <li> <strong>Recall / TPR / Sensitivity:</strong> \(\text{Rec}=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FN}}\)</li> <li> <strong>Specificity / TNR:</strong> \(\frac{\mathrm{TN}}{\mathrm{TN}+\mathrm{FP}}\)</li> <li> <strong>F1:</strong> \(2\frac{\text{Prec}\cdot \text{Rec}}{\text{Prec}+\text{Rec}}\)</li> <li> <strong>ROC curve:</strong> TPR vs. FPR for varying thresholds.</li> <li> <strong>PR curve:</strong> Precision vs. Recall.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Confusion matrix heatmap.</li> <li>ROC and PR curves for three competing methods.</li> </ul> <h2 id="84-generalization-biasvariance">8.4 Generalization: Bias–Variance</h2> <ul> <li> <strong>Bias:</strong> error from restrictive assumptions (underfitting).</li> <li> <strong>Variance:</strong> sensitivity to data fluctuations (overfitting).</li> <li> <strong>Trade‑off:</strong> \(\mathbb{E}[\mathrm{MSE}] = \sigma_{\text{noise}}^2 + \text{bias}^2 + \text{variance}\).</li> <li> <strong>Rules of thumb:</strong> start simple, add model capacity with more data; spend effort on features.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>U‑shaped validation curve vs. model capacity.</li> <li>Training vs. validation error across epochs/capacity.</li> </ul> <h2 id="85-feature-design">8.5 Feature Design</h2> <ul> <li> <strong>Per‑pixel:</strong> intensity, RGB, multispectral/hyperspectral signatures.</li> <li> <strong>Neighborhood:</strong> patch/region statistics → higher‑dimensional features.</li> <li> <strong>Task‑specific:</strong> texture (LBP), gradients (HOG), etc.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Pixel vs. patch feature visualization on a remote‑sensing tile.</li> <li>Hyperspectral signature plot for two classes.</li> </ul> <h2 id="86-nearest-neighbor-nnknn">8.6 Nearest Neighbor (NN/k‑NN)</h2> <ul> <li> <strong>NN:</strong> Voronoi partition; sensitive to noise/scale.</li> <li> <strong>k‑NN:</strong> majority/weighted vote among \(k\) nearest; \(k\) tunes bias–variance.</li> <li> <strong>Distance metrics:</strong> Euclidean, cosine, Mahalanobis (handle class variance).</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Decision regions for NN vs. k‑NN on toy data.</li> <li>Effect of \(k\) on boundary smoothness.</li> </ul> <h2 id="87-decision-trees">8.7 Decision Trees</h2> <ul> <li> <strong>Split nodes:</strong> choose tests that maximize class purity (entropy or Gini).</li> <li> <strong>Leaves:</strong> posterior class distribution or label.</li> <li> <strong>Stopping/pruning:</strong> avoid overfitting.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Tiny tree with axis‑aligned thresholds.</li> <li>Entropy/Gini impurity vs. split threshold plot.</li> </ul> <h2 id="88-support-vector-machines-svms">8.8 Support Vector Machines (SVMs)</h2> <ul> <li> <strong>Hard/soft margin:</strong> maximize margin with slack \(\xi_i\).</li> <li> <strong>Kernel trick:</strong> linear separator in feature space via kernels (RBF, poly).</li> <li> <strong>Decision:</strong> sign of \(w^\top x + b\).</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Max‑margin geometry with margin planes \(H_1,H_2\).</li> <li>Effect of RBF kernel width on decision boundary.</li> </ul> <h2 id="89-map-classification-bayes">8.9 MAP Classification (Bayes)</h2> <ul> <li> <strong>Bayes rule:</strong> \(p(c\mid x)\propto p(x\mid c)\,p(c)\).</li> <li> <strong>MAP decision:</strong> \(\arg\max_c\, p(x\mid c)p(c)\).</li> <li> <strong>Loss/risk:</strong> incorporate class‑dependent penalties; optional reject class.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Gaussian class‑conditional contours with MAP boundary.</li> <li>Risk surface for asymmetric losses.</li> </ul> <h2 id="810-ensembles-bagging-random-forests-boosting-adaboost">8.10 Ensembles: Bagging, Random Forests, Boosting (AdaBoost)</h2> <ul> <li> <strong>Bagging:</strong> resample data, average/vote to reduce variance.</li> <li> <strong>Random Forests:</strong> bagging + random feature subsets per split.</li> <li> <strong>Boosting (AdaBoost):</strong> sequentially reweight errors; combine weak learners.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Pipeline sketches for bagging vs. boosting.</li> <li>Decision‑stump boundaries through boosting rounds; margin growth histogram.</li> </ul> <hr> <h1 id="chapter-9--neural-networks-mlps--learning">Chapter 9 — Neural Networks (MLPs) &amp; Learning</h1> <h2 id="91-from-perceptron-to-mlp">9.1 From Perceptron to MLP</h2> <ul> <li> <strong>Neuron:</strong> \(z=w^\top a + b\), activation \(a'=\phi(z)\) (ReLU, sigmoid, tanh).</li> <li> <strong>MLP:</strong> stacked affine+nonlinear layers; universal approximation.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Single neuron diagram with weights, bias, activation.</li> <li>MLP block: input → hidden layers → output.</li> </ul> <h2 id="92-inputs-outputs-and-encodings">9.2 Inputs, Outputs, and Encodings</h2> <ul> <li> <strong>Images:</strong> vectorize pixels (or keep tensors for CNNs).</li> <li> <strong>Labels:</strong> one‑hot vectors; softmax outputs for multi‑class.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>MNIST “5” with one‑hot target.</li> <li>Softmax bar plot for a sample prediction.</li> </ul> <h2 id="93-forward-pass-in-matrix-form">9.3 Forward Pass in Matrix Form</h2> <p>Layer \(l\): \(z^{(l)}=W^{(l)}a^{(l-1)}+b^{(l)},\quad a^{(l)}=\phi^{(l)}(z^{(l)})\).<br> Inference is linear algebra with nonlinearities between layers.</p> <p><strong>Suggested figures</strong></p> <ul> <li>Computational graph of a 3‑layer MLP with tensors annotated.</li> </ul> <h2 id="94-losses-and-optimization">9.4 Losses and Optimization</h2> <ul> <li> <strong>Losses:</strong> MSE, cross‑entropy.</li> <li> <strong>Gradient Descent/SGD:</strong> \(\theta\leftarrow \theta-\eta\nabla_\theta \mathcal{L}\).</li> <li> <strong>Mini‑batches:</strong> stochastic approximation for scalability.</li> <li> <strong>Optimizers:</strong> momentum, Adam; LR schedules; early stopping; weight decay.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Loss landscape slice with GD steps.</li> <li>Training/validation curves comparing SGD vs. Adam.</li> </ul> <h2 id="95-backpropagation">9.5 Backpropagation</h2> <ul> <li> <strong>Key ideas:</strong> computational graph, chain rule, local gradients.</li> <li> <strong>Forward:</strong> cache intermediates; <strong>backward:</strong> propagate sensitivities to obtain \(\nabla_\theta \mathcal{L}\).</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Tiny graph with forward values and backprop derivatives.</li> <li>Table of layerwise gradient shapes.</li> </ul> <h2 id="96-practicalities">9.6 Practicalities</h2> <ul> <li> <strong>Initialization:</strong> He/Xavier.</li> <li> <strong>Activation choice:</strong> ReLU variants; Batch/Layer/Instance Norm.</li> <li> <strong>Regularization:</strong> dropout, data augmentation, early stopping.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Effect of batch norm on training speed.</li> <li>Overfitting example reduced by dropout.</li> </ul> <hr> <h1 id="chapter-10--convolutional-neural-networks-cnns">Chapter 10 — Convolutional Neural Networks (CNNs)</h1> <h2 id="101-why-cnns">10.1 Why CNNs?</h2> <ul> <li>Preserve spatial structure; learn <strong>local</strong>, translation‑equivariant features.</li> <li>Convolutions + pooling/strides reduce resolution and increase receptive field.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>“Flatten vs. keep tensor” comparison.</li> <li>Receptive field growth across layers.</li> </ul> <h2 id="102-convolution-layers">10.2 Convolution Layers</h2> <ul> <li> <strong>Input tensor:</strong> \(C_\text{in}\times H\times W\).</li> <li> <strong>Kernels:</strong> \(C_\text{out}\) filters of shape \(C_\text{in}\times k\times k\).</li> <li> <strong>Padding/stride:</strong> control output size; “same” vs. “valid”.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Sliding kernel animation (one channel for clarity).</li> <li>Output shape formula diagram with \(k,s,p\).</li> </ul> <h2 id="103-pooling-and-striding">10.3 Pooling and Striding</h2> <ul> <li> <strong>Max/Average pooling:</strong> downsample; translation robustness.</li> <li> <strong>Strided convs:</strong> alternative to pooling.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>2×2 max‑pool example with stride 2.</li> <li>Strided conv vs. pooling comparison grid.</li> </ul> <h2 id="104-architectures--normalization">10.4 Architectures &amp; Normalization</h2> <ul> <li> <strong>LeNet‑5 → AlexNet → VGG → ResNet:</strong> increasing depth, residuals.</li> <li> <strong>Normalization:</strong> batch norm to stabilize and speed training.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Block diagrams of LeNet‑5 and a ResNet residual block.</li> <li>Training curves with/without normalization.</li> </ul> <h2 id="105-training-cnns">10.5 Training CNNs</h2> <ul> <li> <strong>Loss/opt:</strong> as in MLPs; often larger batches; strong augmentation.</li> <li> <strong>Regularization:</strong> weight decay, dropout (often later layers).</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Data augmentation gallery.</li> <li>Accuracy vs. epoch for baseline vs. augmented.</li> </ul> <hr> <h1 id="chapter-11--camera-parameters-dlt-calibration--p3p">Chapter 11 — Camera Parameters, DLT, Calibration &amp; P3P</h1> <h2 id="111-coordinate-systems-and-the-imaging-chain">11.1 Coordinate Systems and the Imaging Chain</h2> <ul> <li> <strong>Spaces:</strong> world/object \((\mathcal{W})\), camera \((\mathcal{C})\), image plane \((\mathcal{I})\), sensor \((\mathcal{S})\).</li> <li> <strong>Pipeline:</strong> world \(\to\) camera (extrinsics) \(\to\) ideal projection \(\to\) sensor mapping \(\to\) non‑linear distortions.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Four coordinate frames sketch with arrows.</li> <li>Pinhole camera geometry with principal point and focal length \(c\).</li> </ul> <h2 id="112-extrinsics-pose">11.2 Extrinsics (Pose)</h2> <ul> <li> <strong>Rigid transform:</strong> \(X_c = R X_w + t\).</li> <li><strong>Homogeneous form:</strong></li> </ul> \[\begin{equation} \begin{bmatrix}X_c\\1\end{bmatrix} = \begin{bmatrix}R &amp; t\\ 0 &amp; 1\end{bmatrix} \begin{bmatrix}X_w\\1\end{bmatrix}. \end{equation}\] <p><strong>Suggested figures</strong></p> <ul> <li>Rotation+translation block matrix with axes annotations.</li> <li>Pose visualization relative to a calibration rig.</li> </ul> <h2 id="113-intrinsics-ideal--affine-camera">11.3 Intrinsics (Ideal &amp; Affine Camera)</h2> <ul> <li> <strong>Ideal pinhole:</strong> \(x_i = \Pi(X_c)=\big[\tfrac{cX_c}{Z_c},\tfrac{cY_c}{Z_c},1\big]^\top\).</li> <li> <strong>Affine/sensor mapping:</strong> principal point \((x_H, y_H)\), pixel scales \(m_x,m_y\), shear \(s\).</li> <li> <strong>Calibration matrix</strong> \(K\):</li> </ul> \[\begin{equation} K=\begin{bmatrix} \alpha_x &amp; s &amp; x_H \\ 0 &amp; \alpha_y &amp; y_H \\ 0 &amp; 0 &amp; 1 \end{bmatrix}, \quad \alpha_x = c\,m_x,\ \alpha_y=c\,m_y. \end{equation}\] <p><strong>Suggested figures</strong></p> <ul> <li>From image plane to pixel coordinates with shift/scale/shear.</li> <li>Matrix \(K\) labeled by parameters.</li> </ul> <h2 id="114-full-projection--dlt">11.4 Full Projection &amp; DLT</h2> <ul> <li> <table> <tbody> <tr> <td> <strong>Projection matrix:</strong> $$ P = K\,[R\</td> <td>\ t] \in \mathbb{R}^{3\times 4} $$.</td> </tr> </tbody> </table> </li> <li> <strong>Homog. mapping:</strong> \(x \sim P X\).</li> <li> <strong>DLT (uncalibrated):</strong> stack linear equations from \((X_i,x_i)\) pairs, solve \(Mp=0\) (SVD) for \(p=\mathrm{vec}(P)\). <ul> <li>Needs \(\ge 6\) non‑coplanar points; rank issues on critical surfaces (coplanar points).</li> </ul> </li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Equation stacking for DLT with a small numeric toy example.</li> <li>Failure case when all control points lie on a plane.</li> </ul> <h2 id="115-nonlinear-distortions">11.5 Non‑Linear Distortions</h2> <ul> <li> <strong>Radial distortion:</strong> \(x_d = x\big(1+k_1 r^2 + k_2 r^4 + \cdots\big)\), \(r=\|x - x_H\|\).</li> <li> <strong>Tangential, thin‑prism…</strong> handled by additional parameters; invert via iteration.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Barrel/pincushion grid before/after rectification.</li> <li>Iterative undistortion schematic (initial guess, update loop).</li> </ul> <h2 id="116-calibration-methods">11.6 Calibration Methods</h2> <h3 id="1161-dltbased-full-calibration">11.6.1 DLT‑based Full Calibration</h3> <ul> <li>Solve \(P\) from \(\ge 6\) 3D–2D correspondences, then decompose \(P\to K,R,t\) via <strong>RQ</strong> (or QR on \(P_{1:3,1:3}^{-1}\)).</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Block showing SVD → \(P\), then RQ → \(K,R\), then \(t\).</li> </ul> <h3 id="1162-zhangs-checkerboard-method-intrinsiconly-first">11.6.2 Zhang’s Checkerboard Method (Intrinsic‑only first)</h3> <ul> <li>For each image, estimate homography \(H\) (planar board: \(Z=0\)).</li> <li>Use constraints on \(H\) to solve a linear system for \(B=K^{-\top}K^{-1}\), then <strong>Cholesky</strong> to recover \(K\).</li> <li>Refine intrinsics + distortion with non‑linear least squares (LM).</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Checkerboard with board/world frame alignment.</li> <li>Matrix flow: points → \(H\) → \(Vb=0\) → \(B\) → \(K\) → non‑linear refinement.</li> </ul> <h2 id="117-p3p--spatial-resection-calibrated">11.7 P3P / Spatial Resection (Calibrated)</h2> <ul> <li> <strong>Given:</strong> intrinsics \(K\), three 3D points \(X_i\) and their image bearings \(u_i\).</li> <li> <strong>Step 1 (rays):</strong> compute unit rays \(d_i = \dfrac{K^{-1}\tilde{x}_i}{\|K^{-1}\tilde{x}_i\|}\).</li> <li> <strong>Step 2 (distances):</strong> apply cosine law on triangle formed by \(X_i\) to solve quartic for depths \(\lambda_i\) (up to 4 solutions).</li> <li> <strong>Step 3 (disambiguation):</strong> use a 4th point or prior to pick the valid solution.</li> <li> <strong>Step 4 (pose):</strong> align \(\{ \lambda_i d_i\}\) to \(\{X_i\}\) via Procrustes/Umeyama to get \(R,t\).</li> <li> <strong>RANSAC:</strong> robustify over many correspondences.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Geometry of three rays to three control points.</li> <li>Quartic solutions plot; selection with a 4th correspondence.</li> <li>Pose alignment diagram (camera frame ↔ world frame).</li> </ul> <h2 id="118-camera-classes--parameter-counts">11.8 Camera Classes &amp; Parameter Counts</h2> <ul> <li>Unit → ideal → Euclidean → affine → general (non‑linear).</li> <li>Parameter counts: \(6\) (pose) + \(5\) (linear intrinsics) + distortion params \(N\).</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Hierarchy ladder diagram with sample matrices and parameter numbers.</li> </ul> <hr> <h2 id="references">References</h2> <ul> <li>C. Stachniss, <em>Photogrammetry I + II</em> (Uni Bonn) — lecture slides &amp; video series - https://www.ipb.uni-bonn.de/teaching/index.html</li> <li>Förstner &amp; Wrobel, <em>Photogrammetric Computer Vision</em>.</li> <li>Szeliski, <em>Computer Vision: Algorithms and Applications</em> (Springer, 2010).</li> <li>Alpaydin, <em>Introduction to Machine Learning</em> (2009).</li> <li>Hartley &amp; Zisserman, <em>Multiple View Geometry in Computer Vision</em> (2004).</li> <li>Goodfellow, Bengio, Courville, <em>Deep Learning</em>.</li> <li>Nielsen, <em>Neural Networks and Deep Learning</em> (online book).</li> <li>Zhang, “A Flexible New Technique for Camera Calibration,” MSR‑TR‑98‑71.</li> <li>Thumbnail image source: https://www.slideshare.net/slideshow/introduction-to-simultaneous-localization-and-mapping-slam-a-presentation-from-skydio/242533355</li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/aerial-geoloc/">Aerial Geo-Localization</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/isaacsim/">Isaac Sim for Drone Simulation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/DroneDesignTools/">Optimized Drone Design</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Radar-ADSB/">Ground Radar and ADS-B for BVLOS Operations</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/RealityCaptureAndPostshot/">Photogrammetry with Google Earth Studio</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Noah Lugon-Moulin. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-posts",title:"Posts",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-aerial-geo-localization",title:"Aerial Geo-Localization",description:"Using aerial imagery generated with Google Earth Studio to test drone localization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/aerial-geoloc/"}},{id:"post-isaac-sim-for-drone-simulation",title:"Isaac Sim for Drone Simulation",description:"Testing out NVIDIA Isaac Sim for drone localization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/isaacsim/"}},{id:"post-photogrammetry-fundamentals-part-1",title:"Photogrammetry Fundamentals - Part 1",description:"Summary notes on Computer Vision, Vision-Based Localization and 3D Reconstruction",section:"Posts",handler:()=>{window.location.href="/blog/2025/photogrammetry-roadmap/"}},{id:"post-optimized-drone-design",title:"Optimized Drone Design",description:"Design a quadcopter given a desired minimum payload mass and minimum flight time",section:"Posts",handler:()=>{window.location.href="/blog/2025/DroneDesignTools/"}},{id:"post-ground-radar-and-ads-b-for-bvlos-operations",title:"Ground Radar and ADS-B for BVLOS Operations",description:"Validating a ground radar's accuracy and its importance for BVLOS drone operations",section:"Posts",handler:()=>{window.location.href="/blog/2025/Radar-ADSB/"}},{id:"post-photogrammetry-with-google-earth-studio",title:"Photogrammetry with Google Earth Studio",description:"Simulating drone imagery with Google Earth Studio and generating a 3D model with RealityCapture",section:"Posts",handler:()=>{window.location.href="/blog/2025/RealityCaptureAndPostshot/"}},{id:"post-integrating-drones-into-wildfire-fighting",title:"Integrating Drones into Wildfire Fighting",description:"Exploring how drone technologies and NASA\u2019s ACERO initiative can enhance wildfire response.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Wildfiredrones/"}},{id:"post-space-and-aerospace-websites-to-check-out",title:"Space and Aerospace Websites to Check Out",description:"A collection of websites related to space and aerospace.",section:"Posts",handler:()=>{window.location.href="/blog/2025/cool-websites/"}},{id:"post-moon-and-mars-landings",title:"Moon and Mars Landings",description:"Visualizing Moon and Mars Landing Locations.",section:"Posts",handler:()=>{window.location.href="/blog/2025/geojson/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"projects-aerial-robotics",title:"Aerial Robotics",description:"Quadcopter autonomously navigating an obstacle course",section:"Projects",handler:()=>{window.location.href="/projects/aerial_project/"}},{id:"projects-aerodynamics",title:"Aerodynamics",description:"Applying potential flow and thin airfoil theory",section:"Projects",handler:()=>{window.location.href="/projects/aerodynamics_project/"}},{id:"projects-3d-vehicle-localization",title:"3D Vehicle Localization",description:"Applied Deep Learning Course",section:"Projects",handler:()=>{window.location.href="/projects/dlav_project/"}},{id:"projects-remote-sensing",title:"Remote Sensing",description:"Mapping Glaciers with Sentinel-2 Imagery and Deep Learning",section:"Projects",handler:()=>{window.location.href="/projects/ipeo_project/"}},{id:"projects-home-robot",title:"Home Robot",description:"Testing out SLAM with an RP LiDAR",section:"Projects",handler:()=>{window.location.href="/projects/jetson_project/"}},{id:"projects-legged-robotics",title:"Legged Robotics",description:"Teaching biped and quadruped robots to walk",section:"Projects",handler:()=>{window.location.href="/projects/leggedrobot_project/"}},{id:"projects-applied-machine-learning",title:"Applied Machine Learning",description:"Implementing fundamental machine learning algorithms",section:"Projects",handler:()=>{window.location.href="/projects/machine_learning_project/"}},{id:"projects-model-predictive-control",title:"Model Predictive Control",description:"Implementing an MPC controller for a rocket prototype",section:"Projects",handler:()=>{window.location.href="/projects/mpc_project/"}},{id:"projects-mobile-robotics",title:"Mobile Robotics",description:"Introductory robotics course",section:"Projects",handler:()=>{window.location.href="/projects/mr_project/"}},{id:"projects-epfl-robot-competition",title:"EPFL Robot Competition",description:"Multidisciplinary project involving the design and build of a robot",section:"Projects",handler:()=>{window.location.href="/projects/robotcomp_project/"}},{id:"projects-water-rocket",title:"Water Rocket",description:"Reusable rocket with parachute recovery",section:"Projects",handler:()=>{window.location.href="/projects/rocket_project/"}},{id:"projects-vision-based-localization",title:"Vision-based Localization",description:"Enhancing localization for a winged drone with computer vision",section:"Projects",handler:()=>{window.location.href="/projects/semester_project/"}},{id:"projects-spacecraft-design-and-systems-engineering",title:"Spacecraft Design and Systems Engineering",description:"Designing a space mission and spacecraft to Enceladus",section:"Projects",handler:()=>{window.location.href="/projects/spacecraft_project/"}},{id:"projects-lunar-rover-localization",title:"Lunar Rover Localization",description:"Master thesis, conducted with NASA Ames Research Center",section:"Projects",handler:()=>{window.location.href="/projects/viper_project/"}},{id:"projects-synthetic-6d-pose-dataset",title:"Synthetic 6D pose dataset",description:"Using Blender for robotic arm manipulation",section:"Projects",handler:()=>{window.location.href="/projects/xplore_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6C%75%67%6F%6E%6E%6F%61%68@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/nlugon","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/benjamin-noah-lugon-moulin-2b171122a","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>
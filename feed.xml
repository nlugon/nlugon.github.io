<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://nlugon.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://nlugon.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-12T03:18:44+00:00</updated><id>https://nlugon.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">Aerial Geo-Localization</title><link href="https://nlugon.github.io/blog/2025/aerial-geoloc/" rel="alternate" type="text/html" title="Aerial Geo-Localization"/><published>2025-11-11T10:00:00+00:00</published><updated>2025-11-11T10:00:00+00:00</updated><id>https://nlugon.github.io/blog/2025/aerial-geoloc</id><content type="html" xml:base="https://nlugon.github.io/blog/2025/aerial-geoloc/"><![CDATA[<h1 id="generating-data-with-google-earth-studio">Generating data with Google Earth Studio</h1> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/aerial-geoloc/aerial-geoloc-ges-480.webp 480w,/assets/img/aerial-geoloc/aerial-geoloc-ges-800.webp 800w,/assets/img/aerial-geoloc/aerial-geoloc-ges-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/aerial-geoloc/aerial-geoloc-ges.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Generating Aerial Imagery with Google Earth Studio" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">Generating Aerial Imagery with Google Earth Studio.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/aerial-geoloc/sample-img-480.webp 480w,/assets/img/aerial-geoloc/sample-img-800.webp 800w,/assets/img/aerial-geoloc/sample-img-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/aerial-geoloc/sample-img.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Sample Image" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">Sample Image.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/aerial-geoloc/ges-3d-tracking-480.webp 480w,/assets/img/aerial-geoloc/ges-3d-tracking-800.webp 800w,/assets/img/aerial-geoloc/ges-3d-tracking-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/aerial-geoloc/ges-3d-tracking.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="3D tracking data" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">3D tracking data generated by Google Earth Engine, providing camera ground truth location.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/aerial-geoloc/img-preproc-480.webp 480w,/assets/img/aerial-geoloc/img-preproc-800.webp 800w,/assets/img/aerial-geoloc/img-preproc-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/aerial-geoloc/img-preproc.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Image Preprocessing" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">Some image pre-processing methods.</p> <h1 id="relative-localization-with-visual-odometry">Relative Localization with Visual Odometry</h1> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/aerial-geoloc/orb-keypoints-480.webp 480w,/assets/img/aerial-geoloc/orb-keypoints-800.webp 800w,/assets/img/aerial-geoloc/orb-keypoints-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/aerial-geoloc/orb-keypoints.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ORB keypoints" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">ORB keypoint extraction.</p> <p>Find distinct features in each image which can then be compared and matched with features in next images.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/aerial-geoloc/keypoint-matches-480.webp 480w,/assets/img/aerial-geoloc/keypoint-matches-800.webp 800w,/assets/img/aerial-geoloc/keypoint-matches-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/aerial-geoloc/keypoint-matches.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Keypoint matching" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">Matching corresponding keypoints accross consecutive frames.</p> <h1 id="absolute-localization-from-satellite-imagery">Absolute Localization from Satellite Imagery</h1> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/aerial-geoloc/copernicus-browser-480.webp 480w,/assets/img/aerial-geoloc/copernicus-browser-800.webp 800w,/assets/img/aerial-geoloc/copernicus-browser-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/aerial-geoloc/copernicus-browser.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Copernicus browser" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">Download satellite imagery through Copernicus browser.</p>]]></content><author><name></name></author><category term="robotics"/><category term="drones"/><category term="robotics"/><category term="autonomy"/><category term="localization"/><category term="gps"/><summary type="html"><![CDATA[Using aerial imagery generated with Google Earth Studio to test drone localization.]]></summary></entry><entry><title type="html">Isaac Sim for Drone Simulation</title><link href="https://nlugon.github.io/blog/2025/isaacsim/" rel="alternate" type="text/html" title="Isaac Sim for Drone Simulation"/><published>2025-10-29T10:00:00+00:00</published><updated>2025-10-29T10:00:00+00:00</updated><id>https://nlugon.github.io/blog/2025/isaacsim</id><content type="html" xml:base="https://nlugon.github.io/blog/2025/isaacsim/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/isaacsim/cover-480.webp 480w,/assets/img/isaacsim/cover-800.webp 800w,/assets/img/isaacsim/cover-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/isaacsim/cover.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Isaac Sim: Indoor Drone Localization (GPS‑Denied)" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">Setting up a drone with onboard depth camera + LiDAR for indoor drone localization in Isaac Sim 5.0. </p> <h2 id="1-goal--context">1) Goal &amp; Context</h2> <p>This post starts documenting how I set up <strong>Isaac Sim 5.0</strong> to prototype <strong>indoor, GPS‑denied drone localization</strong> pipelines. I wanted a way to:</p> <ul> <li>Simulate <strong>autonomous drone navigation</strong>,</li> <li>Visualize <strong>camera</strong> and <strong>LiDAR</strong> sensor data,</li> <li>Test out <strong>GPS‑denied localization</strong> algorithms with both <strong>known‑map</strong> and <strong>unknown‑map</strong> workflows.</li> </ul> <p>This short tutorial video below was a great start for me:</p> <ul> <li><strong>Video:</strong> “Get Started with Isaac Sim 5.0 (pip install)” — <a href="https://www.youtube.com/watch?v=NV2hqw8wu3U">https://www.youtube.com/watch?v=NV2hqw8wu3U</a></li> </ul> <hr/> <h2 id="2-install-isaac-sim-pip">2) Install Isaac Sim (pip)</h2> <h3 id="21-prerequisites-workstation-specs">2.1 Prerequisites (Workstation Specs)</h3> <p>Nvidia lists the following requirements for running Isaac Sim 5.x<sup id="fnref:reqs"><a href="#fn:reqs" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>:</p> <ul> <li><strong>OS:</strong> Ubuntu 22.04/24.04 (Linux x64) or Windows 10/11</li> <li><strong>CPU:</strong> 7th‑gen <strong>Intel i7</strong> / <strong>Ryzen 5</strong> or better (9th‑gen i7 / Ryzen 7 recommended; i9/Ryzen 9 ideal)</li> <li><strong>RAM:</strong> <strong>32 GB minimum</strong> (64 GB recommended)</li> <li><strong>GPU:</strong> Recent <strong>GeForce RTX</strong>/<strong>RTX Ada</strong> or pro‑series RTX, with <strong>≥16 GB VRAM</strong> for heavier scenes</li> <li><strong>Storage:</strong> Fast SSD; dozens of GB free for assets and caches</li> </ul> <p>You can verify your machine using the <a href="https://docs.isaacsim.omniverse.nvidia.com/5.0.0/installation/quick-install.html">Isaac Sim Compatibility Checker</a>. In my case, I have a dual-boot desktop PC with Ubuntu 22.04 installed on its own SSD, and that meets Nvidia’s requirements. I also already had ROS2 Humble installed.</p> <h3 id="22-create-a-python311-environment-ubuntu2204">2.2 Create a Python 3.11 environment (Ubuntu 22.04)</h3> <p>Depending on whether you wish to use Isaac Sim 5.x (latest) or Isaac Sim 4.x (supports relevant plugins such as Pegasus which currently aren’t supported on 5.x yet at time of writing), keep in mind that Isaac Sim 5.x requires Python 3.11, and Isaac Sim 4.x requires Python 3.10.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create a working folder (will create virtual env and pip instal)</span>
<span class="nb">mkdir</span> <span class="nt">-p</span> ~/isaacsim <span class="o">&amp;&amp;</span> <span class="nb">cd</span> ~/isaacsim

<span class="c"># Ensure Python 3.11 is available</span>
<span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> python3.11 python3.11-venv python3.11-dev

<span class="c"># Create &amp; activate a virtual environment</span>
python3.11 <span class="nt">-m</span> venv ~/venvs/isaacsim-5
<span class="nb">source </span>env_isaacsim/bin/activate
python <span class="nt">-V</span>   <span class="c"># expect 3.11.x</span>
</code></pre></div></div> <h3 id="23-install-isaac-sim-50-pip-and-launch">2.3 Install Isaac Sim 5.0 (pip) and Launch</h3> <p>It is also possible to download and install from the Isaac Sim Documentation website, but doing pip install with your virtual environment sourced allows it to be contained in that environment, and if needed you’d be able to easily install another version of Isaac Sim within a separate environment with few issues.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># From inside the venv</span>
pip <span class="nb">install</span> <span class="nt">--upgrade</span> pip
pip <span class="nb">install </span>isaacsim[all,extscache]<span class="o">==</span>5.0.0 <span class="nt">--extra-index-url</span> https://pypi.nvidia.com
</code></pre></div></div> <p>Launch with:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>isaacsim
</code></pre></div></div> <hr/> <h2 id="3-ros2-bridge-setup-humble">3) ROS 2 Bridge Setup (Humble)</h2> <p>I initially had an issue where the <strong>ROS 2 bridge didn’t start</strong> because my system‑installed ROS 2 conflicted with Isaac Sim’s internal ROS libraries. The fix was to <strong>use Isaac Sim’s internal ROS 2 and FastDDS</strong>, and <strong>avoid sourcing</strong> the system ROS2 (which runs Python 3.10) in that terminal.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># (Optional) Activate the venv you installed Isaac Sim into</span>
<span class="nb">source </span>env_isaacsim/bin/activate

<span class="c"># Point Isaac Sim’s ROS 2 bridge at its internal Humble runtime</span>
<span class="nb">export </span><span class="nv">ROS_DISTRO</span><span class="o">=</span>humble
<span class="nb">export </span><span class="nv">RMW_IMPLEMENTATION</span><span class="o">=</span>rmw_fastrtps_cpp

<span class="c"># IMPORTANT: use the internal bridge libs that ship with Isaac Sim</span>
<span class="c"># Replace $ISAACSIM_PY with your site-packages path for the venv</span>
<span class="nb">export </span><span class="nv">ISAACSIM_PY</span><span class="o">=</span><span class="si">$(</span>python <span class="nt">-c</span> <span class="s2">"import site; print(site.getsitepackages()[0])"</span><span class="si">)</span>
<span class="nb">export </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="s2">"</span><span class="nv">$LD_LIBRARY_PATH</span><span class="s2">:</span><span class="nv">$ISAACSIM_PY</span><span class="s2">/isaacsim/exts/isaacsim.ros2.bridge/humble/lib"</span>

<span class="c"># Ensure you DON'T pull in system ROS 2 in this terminal</span>
<span class="nb">unset </span>AMENT_PREFIX_PATH COLCON_PREFIX_PATH

<span class="c"># Launch Isaac Sim</span>
isaacsim
</code></pre></div></div> <blockquote> <p><strong>Also check:</strong> <code class="language-plaintext highlighter-rouge">Window → Extensions</code>, look up <code class="language-plaintext highlighter-rouge">ros2</code> and verify <strong><code class="language-plaintext highlighter-rouge">isaacsim.ros2.bridge</code></strong> is <strong>enabled</strong>.</p> </blockquote> <p>When using separate shells for <code class="language-plaintext highlighter-rouge">rviz2</code>, <code class="language-plaintext highlighter-rouge">rqt</code>, or other ROS2 nodes, start by sourcing ROS2, and ensure you have a <strong>consistent</strong> domain ID:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Source ROS2</span>
<span class="nb">source</span> /opt/ros/humble/setup.bash
<span class="c"># In every ROS 2 shell you’ll use with this sim:</span>
<span class="nb">export </span><span class="nv">ROS_DOMAIN_ID</span><span class="o">=</span>42 <span class="c"># I actually used the default 0 so no need for this</span>
<span class="c"># (Optional) persist it</span>
<span class="nb">echo</span> <span class="s1">'export ROS_DOMAIN_ID=42'</span> <span class="o">&gt;&gt;</span> ~/.bashrc
</code></pre></div></div> <hr/> <h2 id="4-add-an-environment-drone-and-sensors">4) Add an environment, drone, and sensors</h2> <h3 id="41-create-an-environment">4.1 Create an environment</h3> <p>Once the simulator is started up (this can take some time for the first launch of Isaac Sim), create an environment for a drone to interact with. You can start with a Ground Plane with physics/collisions enabled, or browse and import one of Isaac Sim’s environment assets (like its high-fidelity warehouse). You can also 3D scan your own room with your phone (using apps like Polycam or Scaniverse), edit in Blender and literally drag and drop it inside the simulator.</p> <p><strong>In Isaac Sim:</strong><br/> <code class="language-plaintext highlighter-rouge">Create → Environments → Asset Browser</code></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/isaacsim/asset-browser-480.webp 480w,/assets/img/isaacsim/asset-browser-800.webp 800w,/assets/img/isaacsim/asset-browser-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/isaacsim/asset-browser.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Select Asset Browser" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">Create → Environment → Asset Browser. </p> <p>You’ll then be able to browse all of Nvidia’s assets under the Isaac Sim Assets (Beta) tab in the bottom left. Simply drag and drop the warehouse into the stage/viewport.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/isaacsim/import-warehouse-480.webp 480w,/assets/img/isaacsim/import-warehouse-800.webp 800w,/assets/img/isaacsim/import-warehouse-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/isaacsim/import-warehouse.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Isaac Sim: Import Warehouse" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">Drag and drop the warehouse into the stage. </p> <h3 id="42-bring-in-a-reference-quad-iris">4.2 Bring in a reference quad (Iris)</h3> <p>A convenient, open asset is the <strong>Iris</strong> quad from <strong>Pegasus Simulator</strong><sup id="fnref:pegasus"><a href="#fn:pegasus" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> (built on Isaac Sim). Download <code class="language-plaintext highlighter-rouge">iris.usd</code> from the repo <a href="https://github.com/PegasusSimulator/PegasusSimulator/tree/main/extensions/pegasus.simulator/ pegasus/simulator/assets/Robots/Iris">here</a> and add it to your scene. You can also use one of Isaac Sim’s drone assets (like the Crazyflie) or import your own.</p> <p><strong>In Isaac Sim:</strong><br/> <code class="language-plaintext highlighter-rouge">File → Add Reference… → iris.usd</code></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/isaacsim/import-drone-480.webp 480w,/assets/img/isaacsim/import-drone-800.webp 800w,/assets/img/isaacsim/import-drone-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/isaacsim/import-drone.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Import Iris Drone" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">Drag and drop Iris drone. </p> <blockquote> <p>Press <strong>Play</strong>: the drone will fall if it’s not constrained/controlled yet—that’s expected.</p> </blockquote> <h3 id="43-add-sensors-and-parent-them-to-the-body">4.3 Add sensors and parent them to the body</h3> <ol> <li>Under the asset browser, select and drag a <strong>Camera</strong> (RealSense D455) and a <strong>LiDAR sensor</strong> (Hesai XT32 for fun) into the stage.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/isaacsim/import-lidar-480.webp 480w,/assets/img/isaacsim/import-lidar-800.webp 800w,/assets/img/isaacsim/import-lidar-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/isaacsim/import-lidar.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Import LIDAR" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">Browse sensors in Asset Browser, then drag and drop on drone. </p> <ol> <li>In the <strong>Stage</strong> tree, <strong>drag the sensor prims under the drone’s body</strong> (e.g., <code class="language-plaintext highlighter-rouge">/World/Iris/base_link</code>) so they move with the vehicle when sim starts.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/isaacsim/stage-tree-480.webp 480w,/assets/img/isaacsim/stage-tree-800.webp 800w,/assets/img/isaacsim/stage-tree-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/isaacsim/stage-tree.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Stage Tree" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">Drag the imported sensors under the drone's body prim so that they move with the drone during simulation. </p> <ol> <li>You can preview a camera feed directly in Isaac Sim via <strong><code class="language-plaintext highlighter-rouge">Window → Viewport → Viewport 2</code></strong>, click the camera icon and then select the newly imported camera.</li> </ol> <h3 id="44-publish-to-ros2-topics">4.4 Publish to ROS 2 topics</h3> <p>With the <strong>ROS 2 Bridge enabled</strong>, setup ROS2 publishers to stream the camera and LIDAR sensor data. This can be done fairly easily via <strong><code class="language-plaintext highlighter-rouge">Tools → Robotics → ROS2 OmniGraphs → Camera or RTXLidar</code></strong> Make sure to do the following:</p> <ul> <li>Set the <strong><code class="language-plaintext highlighter-rouge">Camera Prim</code></strong> to the path to your actual sensor (with type <code class="language-plaintext highlighter-rouge">Camera</code> or <code class="language-plaintext highlighter-rouge">LIDAR</code>)</li> <li>Set the <strong><code class="language-plaintext highlighter-rouge">Frame ID</code></strong> to either the drone body prim or the sensor’s main prim</li> <li>Optionally set a node name if you’ll be images or lidar data from multiple sensors simultaneously (set node name to <code class="language-plaintext highlighter-rouge">leftcam</code>, <code class="language-plaintext highlighter-rouge">rightcam</code>…)</li> <li>Click OK, and under the stage tree you’ll see a new element called <strong><code class="language-plaintext highlighter-rouge">Graph</code></strong>. Expand <strong><code class="language-plaintext highlighter-rouge">Graph → ROSCamera → Context</code></strong> and double‑check that your <strong><code class="language-plaintext highlighter-rouge">ROS_DOMAIN_ID</code></strong> matches the ROS2 Domain ID you’ll be using in the separate ROS2 shells/terminals.</li> </ul> <p>Additionally, make sure to publish TF data with <strong><code class="language-plaintext highlighter-rouge">Tools → Robotics → ROS2 OmniGraphs → TF Publisher</code></strong> to be able to later visualize 3D data such as point clouds. Start by adding as <code class="language-plaintext highlighter-rouge">Target Prim</code> the <code class="language-plaintext highlighter-rouge">body</code> prim of the drone, and leaving the <code class="language-plaintext highlighter-rouge">Parent Prim</code> empty (defaults to <code class="language-plaintext highlighter-rouge">/World</code>). Click OK, and then create another ROS2 TF Graph with the target being the camera or the LIDAR sensor, and the parent being the body of the drone. Also check ‘Add to an existing graph’ which should add this to the first created world to body TF2 Publisher graph.</p> <blockquote> <p>Historically you had to wire an <strong>Action Graph</strong> by hand; the newer method <strong>auto‑generate</strong> the graph. You can still inspect it under <strong><code class="language-plaintext highlighter-rouge">Window → Action Graph</code></strong>.</p> </blockquote> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/isaacsim/ros2-camera-graph-480.webp 480w,/assets/img/isaacsim/ros2-camera-graph-800.webp 800w,/assets/img/isaacsim/ros2-camera-graph-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/isaacsim/ros2-camera-graph.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ROS2 Omnigraph" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/isaacsim/ros2-context-480.webp 480w,/assets/img/isaacsim/ros2-context-800.webp 800w,/assets/img/isaacsim/ros2-context-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/isaacsim/ros2-context.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ROS2 Context Node" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Create an Action Graph via Tools → Robotics → ROS2 OmniGraphs to publish sensor data through ROS2. Under Context, ensure the ROS2 Domain ID is consistent with the one used in each ROS 2 terminal. </div> <hr/> <h2 id="5-inspect-topics-and-visualize-data">5) Inspect Topics and Visualize Data</h2> <p>In a <strong>new terminal</strong>, run your usual tools:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># source it in a separate terminal, not the sim’s terminal:</span>
<span class="nb">source</span> /opt/ros/humble/setup.bash

<span class="c"># Be sure the domain matches the sim’s terminal</span>
<span class="nb">export </span><span class="nv">ROS_DOMAIN_ID</span><span class="o">=</span>0 <span class="c"># It is already 0 by default</span>

<span class="c"># Examples:</span>
ros2 topic list
ros2 topic <span class="nb">echo</span> /whatever
rviz2
rqt
</code></pre></div></div> <h3 id="51-foxglove">5.1 Foxglove</h3> <p>Foxglove offers a nice app and web viewer for quick dashboard visualization. Follow the Foxglove instructions <a href="https://docs.foxglove.dev/docs/getting-started/frameworks/ros2#foxglove-websocket"><strong>here</strong></a> and enable its ROS 2 bridge:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ros2 launch foxglove_bridge foxglove_bridge_launch.xml
</code></pre></div></div> <p>Then open Foxglove in the browser or app, add the WebSocket connection, and visualize <strong>rgb image</strong>, <strong>depth image</strong>, and <strong>3D data</strong> overlays.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/isaacsim/foxglove-480.webp 480w,/assets/img/isaacsim/foxglove-800.webp 800w,/assets/img/isaacsim/foxglove-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/isaacsim/foxglove.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Foxglove" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">Visualizing Sensor Data and TF Tree in Foxglove. </p> <hr/> <h2 id="6-next-steps">6) Next Steps</h2> <p>Next steps will be to actually control the drone via ROS2 and set waypoints or teleoperate the drone manually to get data for running localization algorithms. And then test out the localization algorithms. Something to figure out as well would be to what extent high-fidelity outdoor environments can be imported (via a Gaussian Splats) for simulating outdoor drone autonomy.</p> <h2 id="7-references">7) References</h2> <ul> <li>Requirements: https://docs.isaacsim.omniverse.nvidia.com/5.1.0/installation/requirements.html</li> <li>Quick install &amp; downloads (5.0.0): https://docs.isaacsim.omniverse.nvidia.com/5.0.0/installation/download.html</li> <li>Compatibility Checker (NGC): https://catalog.ngc.nvidia.com/orgs/nvidia/containers/isaac-sim-comp-check</li> </ul> <ul> <li>https://github.com/PegasusSimulator/PegasusSimulator</li> <li>(Docs) https://pegasussimulator.github.io/PegasusSimulator/</li> </ul> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:reqs"> <p><strong>System Requirements &amp; Compatibility Checker.</strong> NVIDIA Isaac Sim Docs (5.x). See Requirements table and Compatibility Checker app. <a href="#fnref:reqs" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:pegasus"> <p><strong>Pegasus Simulator (Iris USD).</strong> <a href="#fnref:pegasus" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="robotics"/><category term="drones"/><category term="robotics"/><category term="simulation"/><category term="autonomy"/><category term="ros"/><summary type="html"><![CDATA[Testing out NVIDIA Isaac Sim for drone localization.]]></summary></entry><entry><title type="html">Photogrammetry Fundamentals - Part 1</title><link href="https://nlugon.github.io/blog/2025/photogrammetry-roadmap/" rel="alternate" type="text/html" title="Photogrammetry Fundamentals - Part 1"/><published>2025-10-26T08:00:00+00:00</published><updated>2025-10-26T08:00:00+00:00</updated><id>https://nlugon.github.io/blog/2025/photogrammetry-roadmap</id><content type="html" xml:base="https://nlugon.github.io/blog/2025/photogrammetry-roadmap/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/skydio-feature-tracking-480.webp 480w,/assets/img/skydio-feature-tracking-800.webp 800w,/assets/img/skydio-feature-tracking-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/skydio-feature-tracking.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Skydio Feature Tracking" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">Source: Skydio</p> <p>Cameras are an incredibly powerful and important sensor for robotics, as they provide a way to perceive the world in a very similar way we do, using our eyes. Wherever there is a light source, a camera provides a very rich and dense amount of information, from which can not only be recognized what different elements are inside of an environment (object recognition), but also from which can be determined your location inside of the environment (localization), and determined the environment’s 3D structured (3D reconstruction/mapping).</p> <p>Especially when dealing with sequences of images (i.e. videos/live camera feed), the amount of information to process becomes extremely large, and multiple methods and techniques should be known to process all of the data efficiently, particularly if this needs to be processed in real-time and on computation-limited setups. One great resource I came accross is Cyrill Stachniss’ Photogrammetry YouTube lecture series, which goes through all fundamentals of cameras and camera data processing, from image smoothing to visual SLAM. So here are some compact notes on the concepts he covers in each of his lectures, which provide an overview and refresher of all core topics.</p> <h2 id="on-this-page">On this page</h2> <ul> <li><a href="#chapter-1--introduction-to-photogrammetry">Chapter 1 — Introduction to Photogrammetry</a></li> <li><a href="#chapter-2--camera-basics--propagation-of-light">Chapter 2 — Camera Basics &amp; Propagation of Light</a></li> <li><a href="#chapter-3--image-histograms--simple-point-operators">Chapter 3 — Image Histograms &amp; Simple Point Operators</a></li> <li><a href="#chapter-4--histogram-equalization-noise-variance-equalization--binary-images">Chapter 4 — Histogram Equalization, Noise Variance Equalization &amp; Binary Images</a></li> <li><a href="#chapter-5--local-operators-geometric-transforms--cross-correlation">Chapter 5 — Local Operators, Geometric Transforms &amp; Cross Correlation</a></li> <li><a href="#chapter-6--visual-features-keypoints--descriptors">Chapter 6 — Visual Features: Keypoints &amp; Descriptors</a></li> <li><a href="#chapter-7--image-segmentation-with-mean-shift">Chapter 7 — Image Segmentation with Mean Shift</a></li> <li><a href="#chapter-8--introduction-to-classification">Chapter 8 — Introduction to Classification</a></li> <li><a href="#chapter-9--neural-networks">Chapter 9 — Neural Networks</a></li> <li><a href="#chapter-10--math-basics-&amp;-homogeneous-transforms">Chapter 10 — Math Basics &amp; Homogeneous Transforms</a></li> <li> <p><a href="#chapter-11--camera-parameters-calibration-and-P3P">Chapter 11 — Camera Parameters, Calibration, and P3P</a></p> </li> <li><a href="#references">References</a></li> </ul> <hr/> <h1 id="chapter-1--introduction-to-photogrammetry">Chapter 1 — Introduction to Photogrammetry</h1> <p><strong>Why cameras?</strong> Contact-free, dense/cheap/fast capture; human-interpretable; supports dynamics &amp; real-time.<br/> <strong>Limits.</strong> Needs light; measures directional intensity only; occlusions; loss of depth information with 3D to 2D projection; other sensors may be more precise.</p> <p><strong>Key idea.</strong> A camera measures light intensities accross a large array of pixels. Each pixel corresponds to a direction at which the light ray(s) entered the camera<br/> <strong>Pipelines.</strong> <em>Forward:</em> object → physics → intrinsics/extrinsics → image. <em>Inverse:</em> images + models + priors → object geometry/semantics.<br/> <strong>Algorithms are central.</strong> Sensors ≈ eyes; estimation ≈ brain → implement methods.</p> <p><strong>Sensors &amp; apps.</strong> Industrial/consumer/aerial cameras, LiDAR; mapping, orthophotos, city models, cultural heritage, robotics, autonomous driving.</p> <hr/> <h1 id="chapter-2--camera-basics--propagation-of-light">Chapter 2 — Camera Basics &amp; Propagation of Light</h1> <p><strong>What is measured?</strong> Pixels integrate photons over exposure time → <strong>intensity</strong> per <strong>direction</strong> (ray).<br/> <strong>Camera elements.</strong> Lens &amp; <strong>aperture</strong>, <strong>shutter</strong> (rolling/global), <strong>sensor</strong> (photosites), <strong>A/D</strong>, post-processing (incl. <strong>demosaicing</strong>).</p> <p><strong>Color imaging.</strong> Three-chip prism vs <strong>single-chip CFA</strong> (Bayer: 50% G); <strong>demosaicing</strong> adds artifacts (edges/moire).<br/> <strong>Shutter.</strong> <strong>Rolling</strong> (row-wise timing skew) vs <strong>global</strong> (simultaneous). Prefer global for geometry/high-speed scenes.</p> <p><strong>Image formation.</strong> <strong>Pinhole</strong> (one ray per point → sharp but dark). <strong>Thin lens</strong> (inline: \(\tfrac{1}{f} = \tfrac{1}{z} + \tfrac{1}{z'}\)); aperture limits off-axis error.</p> <p><strong>Aperture &amp; DoF.</strong> Higher f-number → more DoF, less light; diffraction at extremes.</p> <p><strong>Lenses &amp; FOV.</strong> Tele (narrow, minimal perspective), normal, wide (perspective exaggeration), <strong>fisheye</strong> (curved lines).<br/> <strong>Aberrations.</strong> <strong>Distortion</strong> (barrel/pincushion/mustache), <strong>spherical</strong>, <strong>chromatic</strong>, <strong>astigmatism</strong>, <strong>coma</strong>, <strong>vignetting</strong>. Mitigate via optics, aperture, calibration.</p> <p><strong>Light models.</strong> <strong>Ray</strong> (Snell/Fresnel, reversibility), <strong>wave</strong> (Maxwell; interference/diffraction; 400–700 nm), <strong>particle</strong> (photons).<br/> <strong>Exposure triangle.</strong> Shutter ↔ motion blur; Aperture ↔ DoF; ISO ↔ noise/gain.</p> <p><em>Suggested figures:</em> pinhole geometry; rolling vs global; exposure triangle.</p> <hr/> <h1 id="chapter-3--image-histograms--simple-point-operators">Chapter 3 — Image Histograms &amp; Simple Point Operators</h1> <p><strong>Image &amp; histogram.</strong> Grayscale image \(I\) is an \(M\times N\) matrix; intensities in \([0, 255]\).<br/> <strong>Histogram</strong> \(h[g]\): count of pixels with intensity \(g\). <strong>CDF</strong> \(H[g]\): cumulative counts/probabilities.</p> <p><strong>What histograms tell.</strong> Brightness (mean), contrast (variance), robustness (median). Shape hints at lighting/contrast, saturation/clipping.<br/> <strong>Compute.</strong> One pass over pixels → \(\mathcal{O}(MN)\). Color images: per-channel histograms.</p> <p><strong>Operators (by support).</strong> Global, <strong>point</strong>, local.<br/> <strong>Point operators.</strong> Map \(g \mapsto f(g)\) independently per pixel (fast via LUT of 256 entries).</p> <p><strong>Linear transform.</strong> \(g' = a\,g + b\).</p> <ul> <li>Adjust brightness (\(b\)) and contrast (\(a\)).</li> <li> <table> <tbody> <tr> <td>Propagation of mean/std: \(\mu' = a\mu + b\), $$ \sigma’ =</td> <td>a</td> <td>\sigma $$.</td> </tr> </tbody> </table> </li> <li>Contrast inversion: \(g' = 255 - g\).</li> </ul> <p><strong>Nonlinear examples.</strong> <strong>Thresholding</strong> → binary image; <strong>quantization</strong> → map to \(K\) discrete levels (for \(K = 2\) gives binary).</p> <p><em>Suggested figures:</em> sample image &amp; histogram; thresholding example; tone curve.</p> <hr/> <h1 id="chapter-4--histogram-equalization-noise-variance-equalization--binary-images">Chapter 4 — Histogram Equalization, Noise Variance Equalization &amp; Binary Images</h1> <h2 id="41-histogram-equalization-he">4.1 Histogram Equalization (HE)</h2> <p><strong>Goal.</strong> Redistribute intensities to <strong>use full dynamic range</strong> and enhance contrast.<br/> <strong>Monotone mapping and PDFs.</strong> For monotone \(b = f(a)\), conservation of area yields</p> \[h_b(b) = h_a(a)\,\left|\frac{da}{db}\right|, \quad a = f^{-1}(b).\] <p><strong>HE solution.</strong> Use the <strong>CDF</strong> of input: \(b = \alpha\,H(a) + \beta\) → approximately uniform output (discrete case not perfectly flat).<br/> <strong>Effect.</strong> Boosts contrast, especially in low-contrast regions; may amplify noise. Variants: <strong>AHE</strong>, <strong>CLAHE</strong> (limits amplification).</p> <p><em>Suggested figures:</em> before/after image &amp; histograms; CDF mapping sketch.</p> <h2 id="42-noise-variance-equalization-nve">4.2 Noise Variance Equalization (NVE)</h2> <p><strong>Photon noise.</strong> Counts follow <strong>Poisson</strong> → mean = variance \(\propto\) intensity; plus constant read noise.<br/> <strong>Goal.</strong> Make variance <strong>independent of intensity</strong>. For variance \(\sigma_g^2 \propto g\) and monotone \(g' = f(g)\), choose \(f\) so that \(\sigma_{g'}^2\) is constant.<br/> <strong>Derivation (sketch).</strong> Variance propagation gives \(f'(g) \propto \tfrac{1}{\sqrt{g}}\), hence</p> \[g' = c\,\sqrt{g} + d\] <p>(<strong>Anscombe-like</strong> transform). Dark regions are stretched; bright compressed.</p> <p><em>Suggested figure:</em> effect of \(\sqrt{\cdot}\) mapping on a gradient.</p> <h2 id="43-binary-images--common-operations">4.3 Binary Images &amp; Common Operations</h2> <p><strong>Binary images.</strong> Pixels in {0,1}. Often obtained via thresholding or segmentation masks.</p> <h3 id="connected-components-cc">Connected components (CC)</h3> <ul> <li><strong>Neighborhoods:</strong> <strong>N4</strong> (up/down/left/right) vs <strong>N8</strong> (plus diagonals).</li> <li><strong>Labeling (grid exploit):</strong> single pass creates provisional labels using processed neighbors; equivalence table resolves merges; second pass compacts labels.</li> <li><strong>Complexity:</strong> linear in the number of foreground pixels.</li> </ul> <h3 id="distance-transform-dt">Distance transform (DT)</h3> <ul> <li><strong>Task:</strong> distance of each pixel to nearest border.</li> <li><strong>Two-pass scheme:</strong> TL→BR then BR→TL, keep min; versions for <strong>N4</strong> and <strong>N8</strong> (under/over-estimate Euclidean).</li> <li><strong>Better approx:</strong> combine straight + diagonal costs; <strong>EDT</strong> (exact) available in libraries.</li> </ul> <h3 id="morphology">Morphology</h3> <ul> <li><strong>Erosion:</strong> shrinks foreground; removes outliers.</li> <li><strong>Dilation:</strong> expands foreground; fills holes.</li> <li><strong>Opening = erosion → dilation:</strong> removes small specks.</li> <li><strong>Closing = dilation → erosion:</strong> fills small holes.</li> <li>Useful to <strong>clean masks</strong> before CC/DT.</li> </ul> <p><em>Suggested figures:</em> CC labeling example; N4 vs N8 DT; opening/closing before-after.</p> <hr/> <h1 id="chapter-5--local-operators-geometric-transforms--cross-correlation">Chapter 5 — Local Operators, Geometric Transforms &amp; Cross Correlation</h1> <blockquote> <p>Local (neighborhood) operators via <strong>convolution</strong>, <strong>geometric image warps + resampling</strong>, and <strong>template matching</strong> with <strong>normalized cross correlation (NCC)</strong>.</p> </blockquote> <h2 id="51-local-operators-via-convolution--smoothing--gradients">5.1 Local Operators via Convolution — Smoothing &amp; Gradients</h2> <p><strong>Operator types.</strong> Point (per‑pixel), <strong>local</strong> (neighborhood), global. Point ops struggle with noise/local structure; <strong>local</strong> ops aggregate neighbors.</p> <p><strong>Convolution.</strong> Discrete 2D: inline: \((f * k)[i,j] = \sum_u \sum_v f[i-u,\,j-v]\,k[u,v]\). Properties: <strong>commutative</strong>, <strong>associative</strong>, <strong>distributive</strong>.<br/> <strong>Separable kernels.</strong> \(k(x,y)=k_x(x)\,k_y(y)\) → two 1D passes (faster).</p> <p><strong>Box filter.</strong> Mean over a window (kernel sums to <strong>1</strong>); reduces noise; border handling: constant, wrap, clamp, mirror.<br/> <strong>Median filter.</strong> Robust to outliers (nonlinear).</p> <p><strong>Binomial/Gaussian smoothing.</strong> Binomial weights (Pascal triangle) ≈ discrete Gaussian; gentler smoothing than box for same window.<br/> <strong>Integral image.</strong> Prefix sums for O(1) box sums per window.</p> <p><strong>Gradients (1st derivatives).</strong> Weighted differences with zero‑sum kernels; in 2D: \(\nabla I = (I_x, I_y)\), magnitude \(|\nabla I|\), direction \(\theta\).<br/> <strong>Sobel (3×3)</strong> = smoothing × derivative; <strong>Scharr</strong> improves gradient <strong>direction</strong> accuracy.<br/> <strong>2nd derivatives.</strong> 1D second difference \(f'' \approx [1, -2, 1]\); <strong>Laplacian</strong> \(\nabla^2 I = I_{xx}+I_{yy}\) for edges.</p> <p><em>Suggested figures:</em> kernel examples (box/binomial), Sobel masks, gradient magnitude image.</p> <h2 id="52-geometric-transformations--resampling">5.2 Geometric Transformations &amp; Resampling</h2> <p><strong>Transforms.</strong> Map output coords \(x'\) to input \(x\) via <strong>inverse warping</strong>: \(x = T^{-1}(x')\). Examples: translation, scale, rotation, affine; rectification, registration, texture mapping.</p> <p><strong>Interpolation.</strong> Assign intensity at non‑integer \(x\):</p> <ul> <li><strong>Nearest neighbor</strong> (fast, blocky), <strong>bilinear</strong> (balanced), <strong>bicubic</strong> (smooth, costlier).</li> <li><strong>Quality vs speed:</strong> NN (++) / (−); Bilinear (+) / (0); Bicubic (−) / (++).</li> </ul> <p><strong>Forward vs inverse warping.</strong> Forward leaves <strong>holes</strong>; <strong>prefer inverse</strong> then interpolate.</p> <p><strong>Subsampling (downscale).</strong> Naive decimation → <strong>aliasing</strong>. Pre‑filter with <strong>binomial/Gaussian</strong> before taking every nth pixel.<br/> <strong>Kernel width &amp; scale.</strong> Smoothing strength depends on kernel std and local scale \(m\) of transform.<br/> <strong>Image pyramids.</strong> Successively downsampled images (×1/2 per level) for multiscale processing.</p> <p><em>Suggested figures:</em> bilinear vs bicubic; forward vs inverse warping; pyramid illustration.</p> <h2 id="53-template-matching-with-normalized-cross-correlation-ncc">5.3 Template Matching with Normalized Cross Correlation (NCC)</h2> <p><strong>Goal.</strong> Find template \(g_2\) in image \(g_1\) under <strong>translation</strong>, allowing brightness/contrast changes.<br/> <strong>NCC</strong> at offset \(\Delta\) (over overlap region):</p> \[\rho(\Delta) = \frac{\sum (g_1 - \mu_1)(g_2 - \mu_2)}{\sigma_1\,\sigma_2},\quad \rho\in[-1,1].\] <p><strong>Search.</strong> Exhaustive over \((\Delta x,\Delta y)\) or <strong>coarse‑to‑fine</strong> with an <strong>image pyramid</strong>.<br/> <strong>Limitations.</strong> Assumes translation only, uniform noise, no occlusion; quality degrades with rotation (\gtrsim 20^\circ) or scale (\gtrsim 30\%).</p> <p><strong>Subpixel refinement.</strong> Fit a <strong>quadratic</strong> around the NCC peak; use gradient/Hessian to estimate argmax → ~0.1 px precision.</p> <p><em>Suggested figures:</em> NCC heatmap with peak; pyramid search schematic.</p> <hr/> <h1 id="chapter-6--visual-features-keypoints--descriptors">Chapter 6 — Visual Features: Keypoints &amp; Descriptors</h1> <blockquote> <p>Find <strong>distinct points</strong> (keypoints) and describe their <strong>local appearance</strong> for matching across images.</p> </blockquote> <h2 id="61-keypoints-corners--blobs">6.1 Keypoints (Corners &amp; Blobs)</h2> <p><strong>Idea.</strong> Corners/texture‑rich spots are stable under translation/rotation/illumination and good for matching.</p> <p><strong>Structure matrix (second‑moment).</strong> From gradients \(I_x, I_y\) (e.g., Sobel/Scharr), build</p> <p>\begin{equation} M = \sum_{(u,v)\in\mathcal{N}} \begin{bmatrix} I_x^2 &amp; I_x I_y <br/> I_x I_y &amp; I_y^2 \end{bmatrix}. \end{equation}</p> <ul> <li><strong>Harris:</strong> \(R = \det(M) - k\,[\mathrm{trace}(M)]^2\) — large \(R\) ⇒ corner.</li> <li><strong>Shi–Tomasi:</strong> threshold the <strong>smallest eigenvalue</strong> \(\lambda_{\min}\).</li> <li><strong>Förstner:</strong> criteria on size/roundness of error ellipse; supports <strong>subpixel</strong> refinement.</li> </ul> <p><strong>Practical steps.</strong> Gray → smooth → gradients → <strong>corner response</strong> → <strong>threshold</strong> → <strong>non‑maximum suppression</strong>.</p> <p><strong>Difference of Gaussians (DoG).</strong> Build a scale‑space pyramid; per level blur with \(\sigma\), subtract consecutive blurs, find <strong>extrema across scales</strong> (corners/blobs). Suppress edge responses via eigenvalue‑ratio test.</p> <p><em>Suggested figure:</em> Harris/Shi–Tomasi responses with NMS (Figure X).</p> <h2 id="62-descriptors-sift-brief-orb">6.2 Descriptors (SIFT, BRIEF, ORB)</h2> <p><strong>Goal.</strong> Represent a keypoint’s neighborhood with a vector <strong>robust</strong> to viewpoint/illumination.</p> <p><strong>SIFT.</strong></p> <ul> <li>Keypoint: DoG extremum with <strong>scale</strong> and <strong>orientation</strong>.</li> <li>Descriptor: <strong>16×16</strong> window at keypoint scale → <strong>4×4</strong> cells × <strong>8‑orient</strong> hist = <strong>128D</strong> (normalized).</li> <li>Invariance: translation/rotation/scale; partial to lighting and affine.</li> <li>Matching: Euclidean distance + <strong>Lowe’s ratio test</strong>.</li> </ul> <p><strong>Binary descriptors.</strong> Fast &amp; compact.</p> <ul> <li><strong>BRIEF:</strong> compare <strong>intensity pairs</strong> in a smoothed patch → bitstring; compare via <strong>Hamming distance</strong>.</li> <li><strong>ORB:</strong> BRIEF + <strong>orientation</strong> (via moments) + <strong>learned pair selection</strong>; rotation‑invariant in‑plane, near real‑time.</li> </ul> <p><strong>Notes.</strong> Keep <strong>consistent pair set &amp; order</strong> for binary descriptors. For scale‑invariance with ORB, use an <strong>image pyramid</strong>.</p> <p><em>Suggested figures:</em> SIFT cell grid and histogram; ORB rotation compensation (Figure X).</p> <hr/> <h1 id="chapter-7--image-segmentation-with-mean-shift">Chapter 7 — Image Segmentation with Mean Shift</h1> <blockquote> <p><strong>Unsupervised</strong> clustering of pixels in a <strong>joint feature space</strong> (e.g., color + position) by following <strong>density modes</strong>.</p> </blockquote> <p><strong>Kernel Density Estimation (KDE).</strong> With kernel \(K\) (often Gaussian) and bandwidths \(h_f, h_s\): estimate local density and compute the <strong>mean shift vector</strong> (center‑of‑mass shift).</p> <p><strong>Algorithm.</strong> 1) Choose kernel &amp; bandwidth(s).<br/> 2) For each pixel feature vector, iteratively <strong>shift window to the local mean</strong> until convergence.<br/> 3) <strong>Merge</strong> windows that converge within thresholds → <strong>regions</strong> (attraction basins).</p> <p><strong>Design.</strong></p> <ul> <li>Feature space: color (e.g., Lab/RGB), optionally <strong>spatial coords</strong>; bandwidths <strong>\(K_f\)</strong> (feature) and <strong>\(K_s\)</strong> (spatial) control granularity.</li> <li>Pros: no preset number of clusters; flexible shapes; robust to outliers.</li> <li>Cons: bandwidth selection matters; scales poorly with <strong>high‑dimensional</strong> features.</li> </ul> <p><strong>Tips.</strong> Use <strong>pyramids</strong> or subsampling for speed; post‑process with <strong>morphology</strong> and <strong>connected components</strong>.</p> <p><em>Suggested figure:</em> mean shift trajectories toward modes; segmentation examples (Figure X).</p> <hr/> <h1 id="chapter-8--introduction-to-classification">Chapter 8 — Introduction to Classification</h1> <h2 id="81-motivation--problem-setup">8.1 Motivation &amp; Problem Setup</h2> <p><strong>Goal.</strong> Given features \(x\) and a finite set of classes \(\mathcal{C}\), learn a mapping \(f: \mathbb{R}^d \rightarrow \mathcal{C}\).<br/> <strong>Examples.</strong> Vegetation/non‑vegetation from pixel spectra; “family car” from price/power.</p> <ul> <li><strong>Supervised learning:</strong> labeled pairs \((x_i, y_i)\).</li> <li><strong>Classification vs. regression:</strong> discrete labels vs. continuous outputs.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Two toy classes in 2‑D feature space with a separating boundary.</li> <li>Table contrasting classification vs. regression (outputs, losses, metrics).</li> </ul> <h2 id="82-hypotheses-version-space-and-margin">8.2 Hypotheses, Version Space, and Margin</h2> <ul> <li><strong>Multiple consistent hypotheses:</strong> same training data, different separators.</li> <li><strong>Version space:</strong> set of hypotheses consistent with the labels.</li> <li><strong>Large margin principle:</strong> prefer separator maximizing distance to support points.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Most‑general vs. most‑specific hypothesis cones; shaded version space.</li> <li>Max‑margin separator with support vectors highlighted.</li> </ul> <h2 id="83-errors-confusion-matrix-and-metrics">8.3 Errors, Confusion Matrix, and Metrics</h2> <p>Let TP, FP, TN, FN denote counts.</p> <ul> <li><strong>Precision:</strong> \(\text{Prec}=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FP}}\)</li> <li><strong>Recall / TPR / Sensitivity:</strong> \(\text{Rec}=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FN}}\)</li> <li><strong>Specificity / TNR:</strong> \(\frac{\mathrm{TN}}{\mathrm{TN}+\mathrm{FP}}\)</li> <li><strong>F1:</strong> \(2\frac{\text{Prec}\cdot \text{Rec}}{\text{Prec}+\text{Rec}}\)</li> <li><strong>ROC curve:</strong> TPR vs. FPR for varying thresholds.</li> <li><strong>PR curve:</strong> Precision vs. Recall.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Confusion matrix heatmap.</li> <li>ROC and PR curves for three competing methods.</li> </ul> <h2 id="84-generalization-biasvariance">8.4 Generalization: Bias–Variance</h2> <ul> <li><strong>Bias:</strong> error from restrictive assumptions (underfitting).</li> <li><strong>Variance:</strong> sensitivity to data fluctuations (overfitting).</li> <li><strong>Trade‑off:</strong> \(\mathbb{E}[\mathrm{MSE}] = \sigma_{\text{noise}}^2 + \text{bias}^2 + \text{variance}\).</li> <li><strong>Rules of thumb:</strong> start simple, add model capacity with more data; spend effort on features.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>U‑shaped validation curve vs. model capacity.</li> <li>Training vs. validation error across epochs/capacity.</li> </ul> <h2 id="85-feature-design">8.5 Feature Design</h2> <ul> <li><strong>Per‑pixel:</strong> intensity, RGB, multispectral/hyperspectral signatures.</li> <li><strong>Neighborhood:</strong> patch/region statistics → higher‑dimensional features.</li> <li><strong>Task‑specific:</strong> texture (LBP), gradients (HOG), etc.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Pixel vs. patch feature visualization on a remote‑sensing tile.</li> <li>Hyperspectral signature plot for two classes.</li> </ul> <h2 id="86-nearest-neighbor-nnknn">8.6 Nearest Neighbor (NN/k‑NN)</h2> <ul> <li><strong>NN:</strong> Voronoi partition; sensitive to noise/scale.</li> <li><strong>k‑NN:</strong> majority/weighted vote among \(k\) nearest; \(k\) tunes bias–variance.</li> <li><strong>Distance metrics:</strong> Euclidean, cosine, Mahalanobis (handle class variance).</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Decision regions for NN vs. k‑NN on toy data.</li> <li>Effect of \(k\) on boundary smoothness.</li> </ul> <h2 id="87-decision-trees">8.7 Decision Trees</h2> <ul> <li><strong>Split nodes:</strong> choose tests that maximize class purity (entropy or Gini).</li> <li><strong>Leaves:</strong> posterior class distribution or label.</li> <li><strong>Stopping/pruning:</strong> avoid overfitting.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Tiny tree with axis‑aligned thresholds.</li> <li>Entropy/Gini impurity vs. split threshold plot.</li> </ul> <h2 id="88-support-vector-machines-svms">8.8 Support Vector Machines (SVMs)</h2> <ul> <li><strong>Hard/soft margin:</strong> maximize margin with slack \(\xi_i\).</li> <li><strong>Kernel trick:</strong> linear separator in feature space via kernels (RBF, poly).</li> <li><strong>Decision:</strong> sign of \(w^\top x + b\).</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Max‑margin geometry with margin planes \(H_1,H_2\).</li> <li>Effect of RBF kernel width on decision boundary.</li> </ul> <h2 id="89-map-classification-bayes">8.9 MAP Classification (Bayes)</h2> <ul> <li><strong>Bayes rule:</strong> \(p(c\mid x)\propto p(x\mid c)\,p(c)\).</li> <li><strong>MAP decision:</strong> \(\arg\max_c\, p(x\mid c)p(c)\).</li> <li><strong>Loss/risk:</strong> incorporate class‑dependent penalties; optional reject class.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Gaussian class‑conditional contours with MAP boundary.</li> <li>Risk surface for asymmetric losses.</li> </ul> <h2 id="810-ensembles-bagging-random-forests-boosting-adaboost">8.10 Ensembles: Bagging, Random Forests, Boosting (AdaBoost)</h2> <ul> <li><strong>Bagging:</strong> resample data, average/vote to reduce variance.</li> <li><strong>Random Forests:</strong> bagging + random feature subsets per split.</li> <li><strong>Boosting (AdaBoost):</strong> sequentially reweight errors; combine weak learners.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Pipeline sketches for bagging vs. boosting.</li> <li>Decision‑stump boundaries through boosting rounds; margin growth histogram.</li> </ul> <hr/> <h1 id="chapter-9--neural-networks-mlps--learning">Chapter 9 — Neural Networks (MLPs) &amp; Learning</h1> <h2 id="91-from-perceptron-to-mlp">9.1 From Perceptron to MLP</h2> <ul> <li><strong>Neuron:</strong> \(z=w^\top a + b\), activation \(a'=\phi(z)\) (ReLU, sigmoid, tanh).</li> <li><strong>MLP:</strong> stacked affine+nonlinear layers; universal approximation.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Single neuron diagram with weights, bias, activation.</li> <li>MLP block: input → hidden layers → output.</li> </ul> <h2 id="92-inputs-outputs-and-encodings">9.2 Inputs, Outputs, and Encodings</h2> <ul> <li><strong>Images:</strong> vectorize pixels (or keep tensors for CNNs).</li> <li><strong>Labels:</strong> one‑hot vectors; softmax outputs for multi‑class.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>MNIST “5” with one‑hot target.</li> <li>Softmax bar plot for a sample prediction.</li> </ul> <h2 id="93-forward-pass-in-matrix-form">9.3 Forward Pass in Matrix Form</h2> <p>Layer \(l\): \(z^{(l)}=W^{(l)}a^{(l-1)}+b^{(l)},\quad a^{(l)}=\phi^{(l)}(z^{(l)})\).<br/> Inference is linear algebra with nonlinearities between layers.</p> <p><strong>Suggested figures</strong></p> <ul> <li>Computational graph of a 3‑layer MLP with tensors annotated.</li> </ul> <h2 id="94-losses-and-optimization">9.4 Losses and Optimization</h2> <ul> <li><strong>Losses:</strong> MSE, cross‑entropy.</li> <li><strong>Gradient Descent/SGD:</strong> \(\theta\leftarrow \theta-\eta\nabla_\theta \mathcal{L}\).</li> <li><strong>Mini‑batches:</strong> stochastic approximation for scalability.</li> <li><strong>Optimizers:</strong> momentum, Adam; LR schedules; early stopping; weight decay.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Loss landscape slice with GD steps.</li> <li>Training/validation curves comparing SGD vs. Adam.</li> </ul> <h2 id="95-backpropagation">9.5 Backpropagation</h2> <ul> <li><strong>Key ideas:</strong> computational graph, chain rule, local gradients.</li> <li><strong>Forward:</strong> cache intermediates; <strong>backward:</strong> propagate sensitivities to obtain \(\nabla_\theta \mathcal{L}\).</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Tiny graph with forward values and backprop derivatives.</li> <li>Table of layerwise gradient shapes.</li> </ul> <h2 id="96-practicalities">9.6 Practicalities</h2> <ul> <li><strong>Initialization:</strong> He/Xavier.</li> <li><strong>Activation choice:</strong> ReLU variants; Batch/Layer/Instance Norm.</li> <li><strong>Regularization:</strong> dropout, data augmentation, early stopping.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Effect of batch norm on training speed.</li> <li>Overfitting example reduced by dropout.</li> </ul> <hr/> <h1 id="chapter-10--convolutional-neural-networks-cnns">Chapter 10 — Convolutional Neural Networks (CNNs)</h1> <h2 id="101-why-cnns">10.1 Why CNNs?</h2> <ul> <li>Preserve spatial structure; learn <strong>local</strong>, translation‑equivariant features.</li> <li>Convolutions + pooling/strides reduce resolution and increase receptive field.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>“Flatten vs. keep tensor” comparison.</li> <li>Receptive field growth across layers.</li> </ul> <h2 id="102-convolution-layers">10.2 Convolution Layers</h2> <ul> <li><strong>Input tensor:</strong> \(C_\text{in}\times H\times W\).</li> <li><strong>Kernels:</strong> \(C_\text{out}\) filters of shape \(C_\text{in}\times k\times k\).</li> <li><strong>Padding/stride:</strong> control output size; “same” vs. “valid”.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Sliding kernel animation (one channel for clarity).</li> <li>Output shape formula diagram with \(k,s,p\).</li> </ul> <h2 id="103-pooling-and-striding">10.3 Pooling and Striding</h2> <ul> <li><strong>Max/Average pooling:</strong> downsample; translation robustness.</li> <li><strong>Strided convs:</strong> alternative to pooling.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>2×2 max‑pool example with stride 2.</li> <li>Strided conv vs. pooling comparison grid.</li> </ul> <h2 id="104-architectures--normalization">10.4 Architectures &amp; Normalization</h2> <ul> <li><strong>LeNet‑5 → AlexNet → VGG → ResNet:</strong> increasing depth, residuals.</li> <li><strong>Normalization:</strong> batch norm to stabilize and speed training.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Block diagrams of LeNet‑5 and a ResNet residual block.</li> <li>Training curves with/without normalization.</li> </ul> <h2 id="105-training-cnns">10.5 Training CNNs</h2> <ul> <li><strong>Loss/opt:</strong> as in MLPs; often larger batches; strong augmentation.</li> <li><strong>Regularization:</strong> weight decay, dropout (often later layers).</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Data augmentation gallery.</li> <li>Accuracy vs. epoch for baseline vs. augmented.</li> </ul> <hr/> <h1 id="chapter-11--camera-parameters-dlt-calibration--p3p">Chapter 11 — Camera Parameters, DLT, Calibration &amp; P3P</h1> <h2 id="111-coordinate-systems-and-the-imaging-chain">11.1 Coordinate Systems and the Imaging Chain</h2> <ul> <li><strong>Spaces:</strong> world/object \((\mathcal{W})\), camera \((\mathcal{C})\), image plane \((\mathcal{I})\), sensor \((\mathcal{S})\).</li> <li><strong>Pipeline:</strong> world \(\to\) camera (extrinsics) \(\to\) ideal projection \(\to\) sensor mapping \(\to\) non‑linear distortions.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Four coordinate frames sketch with arrows.</li> <li>Pinhole camera geometry with principal point and focal length \(c\).</li> </ul> <h2 id="112-extrinsics-pose">11.2 Extrinsics (Pose)</h2> <ul> <li><strong>Rigid transform:</strong> \(X_c = R X_w + t\).</li> <li><strong>Homogeneous form:</strong></li> </ul> \[\begin{equation} \begin{bmatrix}X_c\\1\end{bmatrix} = \begin{bmatrix}R &amp; t\\ 0 &amp; 1\end{bmatrix} \begin{bmatrix}X_w\\1\end{bmatrix}. \end{equation}\] <p><strong>Suggested figures</strong></p> <ul> <li>Rotation+translation block matrix with axes annotations.</li> <li>Pose visualization relative to a calibration rig.</li> </ul> <h2 id="113-intrinsics-ideal--affine-camera">11.3 Intrinsics (Ideal &amp; Affine Camera)</h2> <ul> <li><strong>Ideal pinhole:</strong> \(x_i = \Pi(X_c)=\big[\tfrac{cX_c}{Z_c},\tfrac{cY_c}{Z_c},1\big]^\top\).</li> <li><strong>Affine/sensor mapping:</strong> principal point \((x_H, y_H)\), pixel scales \(m_x,m_y\), shear \(s\).</li> <li><strong>Calibration matrix</strong> \(K\):</li> </ul> \[\begin{equation} K=\begin{bmatrix} \alpha_x &amp; s &amp; x_H \\ 0 &amp; \alpha_y &amp; y_H \\ 0 &amp; 0 &amp; 1 \end{bmatrix}, \quad \alpha_x = c\,m_x,\ \alpha_y=c\,m_y. \end{equation}\] <p><strong>Suggested figures</strong></p> <ul> <li>From image plane to pixel coordinates with shift/scale/shear.</li> <li>Matrix \(K\) labeled by parameters.</li> </ul> <h2 id="114-full-projection--dlt">11.4 Full Projection &amp; DLT</h2> <ul> <li> <table> <tbody> <tr> <td><strong>Projection matrix:</strong> $$ P = K\,[R\</td> <td>\ t] \in \mathbb{R}^{3\times 4} $$.</td> </tr> </tbody> </table> </li> <li><strong>Homog. mapping:</strong> \(x \sim P X\).</li> <li><strong>DLT (uncalibrated):</strong> stack linear equations from \((X_i,x_i)\) pairs, solve \(Mp=0\) (SVD) for \(p=\mathrm{vec}(P)\). <ul> <li>Needs \(\ge 6\) non‑coplanar points; rank issues on critical surfaces (coplanar points).</li> </ul> </li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Equation stacking for DLT with a small numeric toy example.</li> <li>Failure case when all control points lie on a plane.</li> </ul> <h2 id="115-nonlinear-distortions">11.5 Non‑Linear Distortions</h2> <ul> <li><strong>Radial distortion:</strong> \(x_d = x\big(1+k_1 r^2 + k_2 r^4 + \cdots\big)\), \(r=\|x - x_H\|\).</li> <li><strong>Tangential, thin‑prism…</strong> handled by additional parameters; invert via iteration.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Barrel/pincushion grid before/after rectification.</li> <li>Iterative undistortion schematic (initial guess, update loop).</li> </ul> <h2 id="116-calibration-methods">11.6 Calibration Methods</h2> <h3 id="1161-dltbased-full-calibration">11.6.1 DLT‑based Full Calibration</h3> <ul> <li>Solve \(P\) from \(\ge 6\) 3D–2D correspondences, then decompose \(P\to K,R,t\) via <strong>RQ</strong> (or QR on \(P_{1:3,1:3}^{-1}\)).</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Block showing SVD → \(P\), then RQ → \(K,R\), then \(t\).</li> </ul> <h3 id="1162-zhangs-checkerboard-method-intrinsiconly-first">11.6.2 Zhang’s Checkerboard Method (Intrinsic‑only first)</h3> <ul> <li>For each image, estimate homography \(H\) (planar board: \(Z=0\)).</li> <li>Use constraints on \(H\) to solve a linear system for \(B=K^{-\top}K^{-1}\), then <strong>Cholesky</strong> to recover \(K\).</li> <li>Refine intrinsics + distortion with non‑linear least squares (LM).</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Checkerboard with board/world frame alignment.</li> <li>Matrix flow: points → \(H\) → \(Vb=0\) → \(B\) → \(K\) → non‑linear refinement.</li> </ul> <h2 id="117-p3p--spatial-resection-calibrated">11.7 P3P / Spatial Resection (Calibrated)</h2> <ul> <li><strong>Given:</strong> intrinsics \(K\), three 3D points \(X_i\) and their image bearings \(u_i\).</li> <li><strong>Step 1 (rays):</strong> compute unit rays \(d_i = \dfrac{K^{-1}\tilde{x}_i}{\|K^{-1}\tilde{x}_i\|}\).</li> <li><strong>Step 2 (distances):</strong> apply cosine law on triangle formed by \(X_i\) to solve quartic for depths \(\lambda_i\) (up to 4 solutions).</li> <li><strong>Step 3 (disambiguation):</strong> use a 4th point or prior to pick the valid solution.</li> <li><strong>Step 4 (pose):</strong> align \(\{ \lambda_i d_i\}\) to \(\{X_i\}\) via Procrustes/Umeyama to get \(R,t\).</li> <li><strong>RANSAC:</strong> robustify over many correspondences.</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Geometry of three rays to three control points.</li> <li>Quartic solutions plot; selection with a 4th correspondence.</li> <li>Pose alignment diagram (camera frame ↔ world frame).</li> </ul> <h2 id="118-camera-classes--parameter-counts">11.8 Camera Classes &amp; Parameter Counts</h2> <ul> <li>Unit → ideal → Euclidean → affine → general (non‑linear).</li> <li>Parameter counts: \(6\) (pose) + \(5\) (linear intrinsics) + distortion params \(N\).</li> </ul> <p><strong>Suggested figures</strong></p> <ul> <li>Hierarchy ladder diagram with sample matrices and parameter numbers.</li> </ul> <hr/> <h2 id="references">References</h2> <ul> <li>C. Stachniss, <em>Photogrammetry I + II</em> (Uni Bonn) — lecture slides &amp; video series - https://www.ipb.uni-bonn.de/teaching/index.html</li> <li>Förstner &amp; Wrobel, <em>Photogrammetric Computer Vision</em>.</li> <li>Szeliski, <em>Computer Vision: Algorithms and Applications</em> (Springer, 2010).</li> <li>Alpaydin, <em>Introduction to Machine Learning</em> (2009).</li> <li>Hartley &amp; Zisserman, <em>Multiple View Geometry in Computer Vision</em> (2004).</li> <li>Goodfellow, Bengio, Courville, <em>Deep Learning</em>.</li> <li>Nielsen, <em>Neural Networks and Deep Learning</em> (online book).</li> <li>Zhang, “A Flexible New Technique for Camera Calibration,” MSR‑TR‑98‑71.</li> <li>Thumbnail image source: https://www.slideshare.net/slideshow/introduction-to-simultaneous-localization-and-mapping-slam-a-presentation-from-skydio/242533355</li> </ul>]]></content><author><name></name></author><category term="robotics"/><category term="camera"/><category term="photogrammetry"/><category term="computer-vision"/><category term="slam"/><category term="visual-odometry"/><summary type="html"><![CDATA[Summary notes on Computer Vision, Vision-Based Localization and 3D Reconstruction]]></summary></entry><entry><title type="html">Optimized Drone Design</title><link href="https://nlugon.github.io/blog/2025/DroneDesignTools/" rel="alternate" type="text/html" title="Optimized Drone Design"/><published>2025-07-08T10:00:00+00:00</published><updated>2025-07-08T10:00:00+00:00</updated><id>https://nlugon.github.io/blog/2025/DroneDesignTools</id><content type="html" xml:base="https://nlugon.github.io/blog/2025/DroneDesignTools/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/drone-design/hovertime-vs-capacity-480.webp 480w,/assets/img/drone-design/hovertime-vs-capacity-800.webp 800w,/assets/img/drone-design/hovertime-vs-capacity-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/drone-design/hovertime-vs-capacity.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Optimized Quadcopter Design" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">Hover Time vs Battery Capacity for different battery types and number of cells. This plot was obtained from applying fairly simplistic physics to large combinations of different propellers, motors and battery specifications (with fairly realistic catalogs of propellers, motors and batteries being synthetically generated with AI).</p> <h2 id="1-goal">1) Goal</h2> <p><strong>Design a quadcopter</strong> that satisfies:</p> <ul> <li><strong>Payload</strong> ≥ target (g)</li> <li><strong>Flight time</strong> ≥ target (min)</li> <li><strong>Total mass</strong> ≤ limit (g)</li> <li><strong>Total cost</strong> ≤ budget (USD)</li> </ul> <p><strong>Output:</strong> Feasible configurations (propeller, motor, battery) with hover/max/burst metrics &amp; plots. Among these configurations, find an optimal solution that also takes into account other factors (agility, thrust-to-weight ratio, average battery discharge rate, …)</p> <hr/> <h2 id="2-approach">2) Approach</h2> <h3 id="21-simplifying-problem">2.1 Simplifying problem</h3> <ul> <li>Each component affects the other components -&gt; not so straightforward to just come up with an analytical formula. So one idea would be to simulate the performance of a large amount of combination of different components, and do some data analysis to figure out what combinations best meet our requirements.</li> <li>Main components we consider for design: <strong>propellers</strong>, <strong>motors</strong>, <strong>battery</strong>.</li> <li>Pretty much ignore everything else (can set an approximate mass for frame, wiring, avionics, … based on the propeller size).</li> <li>Assume <strong>4 rotors</strong> (quadcopter).</li> </ul> <h3 id="22-datasets-synthetic-but-realistic">2.2 Datasets (synthetic but realistic)</h3> <p>Use AI to generate plausible catalogs of:</p> <ul> <li><strong>Propellers</strong>: diameter (5–33″), pitch, blades, mass, inertia, <strong>thrust vs RPM</strong> curve, unit cost.</li> <li><strong>Motors</strong>: size (e.g., 1404–7215), KV, Kt, mass, nominal/max currents, voltage range, acceptable prop diameter range, cost.</li> <li><strong>Batteries</strong>: capacity (mAh), cells (S), C‑rating, chemistry (LiPo/Li‑ion), mass, cost.</li> </ul> <p>Files are JSON: <code class="language-plaintext highlighter-rouge">propellers.json</code>, <code class="language-plaintext highlighter-rouge">motors.json</code>, <code class="language-plaintext highlighter-rouge">batteries.json</code>.</p> <h3 id="23-algorithm-simplified-find-combinations-that-satisfy-requirements">2.3 Algorithm (simplified): Find combinations that satisfy requirements</h3> <p>For each (prop, motor, battery) combination:</p> <p>TODO</p> <hr/> <h2 id="3-results">3) Results</h2> <p>TODO</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/drone-design/hovertime-vs-capacity-480.webp 480w,/assets/img/drone-design/hovertime-vs-capacity-800.webp 800w,/assets/img/drone-design/hovertime-vs-capacity-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/drone-design/hovertime-vs-capacity.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Optimized Quadcopter Design" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">Hover Time vs Battery Capacity for different battery types and number of cells. Note that this plot relies on multiple simplified assumptions and also relies on specs for props/motors/batteries that were synthetically generated (prompted to be as realistic as possible, but may can differ to current off-the-shelf parts).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/drone-design/twr-vs-capacity-480.webp 480w,/assets/img/drone-design/twr-vs-capacity-800.webp 800w,/assets/img/drone-design/twr-vs-capacity-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/drone-design/twr-vs-capacity.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Optimized Quadcopter Design" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">Thrust-to-Weight ratio vs Battery Capacity for different battery types and number of cells.</p> <h3 id="discussion">Discussion</h3> <h4 id="aspects-that-look-goodmake-sense">Aspects that look good/make sense:</h4> <ul> <li>Liion batteries lead to longer flight times that LiPo (expected)</li> <li>Higher cell-count seems to lead to longer flight times. Note that higher cell-count should also mean more heavy drone, which would then decrease the flight time,</li> <li>A too large battery capacity actually ends up in decreased flight time compared to a smaller capacity battery (too much weight which the current prop/motor/battery configuration may no longer be good for)</li> </ul> <hr/> <h2 id="4-next-steps">4) Next steps</h2> <h3 id="improvements-needed-for-current-approach">Improvements needed for current approach:</h3> <ul> <li>Replace synthetic entries with actual vendor data and also keep track of links and part availability.</li> <li>Add more complexity in the physics to take into account non-linear battery drainage, motor I₀, winding resistance Rₘ, ESC efficiency, wiring drops, …</li> </ul> <h3 id="broaden-application">Broaden application</h3> <ul> <li><strong>Multicopters</strong>: generalize rotor count (tri, hexa, octo), consider having two propellers above each other (the turbulent air on the propeller below will lead to less generated thrust)</li> <li><strong>VTOL / Fixed‑wing</strong>: add aerodynamic power &amp; propulsive efficiency vs airspeed.</li> </ul> <hr/>]]></content><author><name></name></author><category term="drone-tools"/><category term="drones"/><category term="tools"/><category term="flight-time"/><category term="battery"/><summary type="html"><![CDATA[Design a quadcopter given a desired minimum payload mass and minimum flight time]]></summary></entry><entry><title type="html">Ground Radar and ADS-B for BVLOS Operations</title><link href="https://nlugon.github.io/blog/2025/Radar-ADSB/" rel="alternate" type="text/html" title="Ground Radar and ADS-B for BVLOS Operations"/><published>2025-06-28T08:00:00+00:00</published><updated>2025-06-28T08:00:00+00:00</updated><id>https://nlugon.github.io/blog/2025/Radar-ADSB</id><content type="html" xml:base="https://nlugon.github.io/blog/2025/Radar-ADSB/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/HiddenLevel-480.webp 480w,/assets/img/HiddenLevel-800.webp 800w,/assets/img/HiddenLevel-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/HiddenLevel.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="HiddenLevel-RadarAndADSB" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">Overlay of radar detections (V shapes) and ADS-B tracks (aircraft shapes). Credits: Hidden Level</p> <p>As <strong>unmanned aerial systems (UAS)</strong> expand into <strong>Beyond Visual Line of Sight (BVLOS)</strong> missions, whether for drone deliveries, infrastructure inspection, or emergency response, ensuring <strong>full airspace awareness</strong> becomes mission-critical. One project I worked on tackled this challenge by comparing data from a ground-based radar with ADS-B (Automatic Dependent Surveillance–Broadcast), with the goal of assessing the radar’s tracking accuracy as part of a redundant airspace surveillance system, which would strengthen the situational awareness needed for autonomous UAS to complete operations safely.</p> <hr/> <h3 id="what-i-worked-on">What I Worked On</h3> <p>The <strong>high-level objective</strong> of the project was to validate the <strong>accuracy</strong> of a <strong>ground-based radar surveillance system</strong> that integrates with other sensing layers—ADS-B and Remote ID—in order to deliver a full airspace picture to BVLOS drone operations. This system aims to ensure that autonomous drones can operate safely, even in environments with mixed or unreliable signal availability.</p> <p><strong>At my level</strong>, my focus was on <strong>characterizing the accuracy and coverage of the radar</strong>. Using Python, I processed large datasets from both the radar and nearby ADS-B receivers. Main tasks included:</p> <ul> <li><strong>Matching radar and ADS-B tracks</strong> based on timestamp and 3D spatial proximity to evaluate the radar’s positional accuracy.</li> <li><strong>Identifying radar detections</strong> that had <strong>no corresponding ADS-B track</strong>—likely representing non-cooperative aircraft, drones, or possible noisy observations.</li> <li><strong>Determining which ADS-B tracks were not detected</strong> by the radar, potentially due to occlusion, distance, or potential other factors.</li> </ul> <p>Beyond these analyses, I also examined <strong>what environmental and contextual factors</strong> could influence the radar’s performance. By correlating accuracy and detection gaps with <strong>weather data, time of day, and geographical areas</strong>, I aimed to understand the <strong>limitations</strong> and <strong>potential optimization areas for deploying such radar systems</strong>.</p> <hr/> <h3 id="why-we-need-ground-radar">Why We Need Ground Radar</h3> <p>While ADS-B is the dominant method for tracking large aircraft, it’s not without failure modes:</p> <ul> <li>A transponder may malfunction.</li> <li>Certain aircraft—such as military or uncooperative planes—may intentionally fly without broadcasting.</li> <li>Even when functioning, ADS-B reception can be obstructed or interfered with.</li> </ul> <p>Meanwhile, <strong>drones are not allowed to broadcast ADS-B</strong> under FAA Part 107 regulations. Instead, they must comply with <strong>Remote ID</strong> requirements—but Remote ID has its own challenges:</p> <ul> <li>The broadcast range is limited, and its effectiveness over long distances is currently a bit uncertain.</li> <li>Not all drones may comply or would need to comply, in the case of sub 250g drones.</li> <li>Remote ID, like ADS-B, depends on proper functioning hardware and line-of-sight.</li> </ul> <p>Because of these limitations, <strong>relying solely on ADS-B and Remote ID is not enough</strong>. This is where <strong>ground radar adds essential redundancy</strong>. It doesn’t depend on broadcast cooperation, and is capable of detecting flying objects within line-of-sight—whether cooperative or untracked. This redundancy is valuable not only for detecting <strong>uncooperative aircraft</strong>, but also for identifying <strong>low-altitude drones and unknown aerial activity</strong>—critical for safe integration of drones into shared airspace.</p> <hr/> <h3 id="going-further-towards-scalable-and-resilient-bvlos-airspace-awareness">Going Further: Towards Scalable and Resilient BVLOS Airspace Awareness</h3> <p>As drone adoption continues to grow—especially for autonomous and BVLOS operations—the airspace, particularly in and around urban areas, is expected to become more <strong>congested and dynamic</strong>. Ensuring the safety of these operations requires that every autonomous aircraft has access to real-time information about:</p> <ul> <li><strong>Crewed aircraft</strong> via ADS-B</li> <li><strong>Nearby drones</strong> via Remote ID</li> <li><strong>Non-cooperative or unexpected traffic</strong> via radar</li> </ul> <p>No single system is sufficient. A comprehensive solution will depend on <strong>multi-layered sensing architectures</strong>, where radar acts not as a backup but as a <strong>critical redundant layer</strong> alongside broadcast-based systems.</p> <p>To support this, we’ll need:</p> <ul> <li><strong>More ground radars</strong>—strategically positioned to minimize blind zones and maximize coverage</li> <li><strong>Optimized deployment locations</strong>, accounting for terrain, urban clutter, and electromagnetic interference</li> <li><strong>Sensor fusion frameworks</strong> that combine radar, ADS-B, and Remote ID into a <strong>unified airspace picture</strong></li> </ul> <p>However, a key challenge arises: BVLOS drones relying on ground-based surveillance must be able to <strong>receive that data in real time</strong>. If the communication link is lost, the drone loses its situational awareness, creating a safety risk.</p> <p>This leads to an additional sensing level but also important tradeoff:</p> <ul> <li><strong>Ground-based awareness provides long-range, centralized detection</strong>, but introduces communication dependency</li> <li><strong>Onboard sensors</strong> (e.g. lightweight radar, cameras, acoustic or vision-based detect-and-avoid) offer <strong>resilience</strong>, but come with <strong>cost, weight, and power tradeoffs</strong></li> </ul> <p>For maximum safety, <strong>future BVLOS drones should incorporate both</strong>:</p> <ul> <li><strong>Live access to the shared surveillance layer</strong> (ADS-B, Remote ID, ground radar)</li> <li><strong>Local onboard sensing</strong> for additional awareness and collision avoidance</li> </ul> <p>This hybrid approach ensures that even if one layer fails—due to signal loss, sensor occlusion, or equipment malfunction—the drone still retains some level of autonomous awareness, enabling it to react and deconflict.</p> <hr/> <h4 id="image-credits">Image Credits:</h4> <p><a href="https://www.hiddenlevel.com/press/hidden-level-awarded-u-s-air-force-phase-1-sttr-contract">Hidden Level</a></p>]]></content><author><name></name></author><category term="aerospace"/><category term="radar"/><category term="adsb"/><category term="drones"/><category term="air-traffic-control"/><category term="systems-engineering"/><summary type="html"><![CDATA[Validating a ground radar's accuracy and its importance for BVLOS drone operations]]></summary></entry><entry><title type="html">Photogrammetry with Google Earth Studio</title><link href="https://nlugon.github.io/blog/2025/RealityCaptureAndPostshot/" rel="alternate" type="text/html" title="Photogrammetry with Google Earth Studio"/><published>2025-06-27T10:00:00+00:00</published><updated>2025-06-27T10:00:00+00:00</updated><id>https://nlugon.github.io/blog/2025/RealityCaptureAndPostshot</id><content type="html" xml:base="https://nlugon.github.io/blog/2025/RealityCaptureAndPostshot/"><![CDATA[<p>Recently explored the potential of <strong>Google Earth Studio</strong> as a tool for <strong>planning and previewing drone photogrammetry workflows</strong>. Around the same time, I was interested in creating small 3D-printed models of notable infrastructure at NASA Ames, such as the <strong>NFAC (the world’s largest wind tunnel)</strong>. Since Google Earth already provides detailed 3D terrain and structure data, I thought it would be interesting to test whether Google Earth Studio could serve as a simulation tool for photogrammetry.</p> <p>Getting started was fairly straightforward. After generating a short animation, I used <strong>RealityCapture</strong> to convert the footage into a textured mesh, which I then cleaned up in <strong>Blender</strong> and prepared for 3D printing. As a secondary experiment, I also used <strong>Jawset PostShot</strong> to generate a Gaussian Splat from the same footage. While the splat wasn’t perfect due to limited sky coverage in the animation, the overall results were compelling. Here’s a quick overview of the workflow and results.</p> <hr/> <h4 id="step-1-google-earth-studio--generate-animation">Step 1: Google Earth Studio – Generate Animation</h4> <p><a href="https://www.google.com/earth/studio/">Google Earth Studio</a> is a browser-based animation tool from Google that allows you to create cinematic flyovers of real-world locations. Documentation is available <a href="https://earth.google.com/studio/docs/">here</a>. To access it:</p> <ul> <li>Go to the <a href="https://www.google.com/earth/studio/">Google Earth Studio website</a></li> <li>Click <strong>Try Earth Studio</strong></li> <li>Sign in with a Google account and briefly describe your intended use</li> </ul> <p>I used the spiral orbit template centered on the NFAC. It’s possible to import <code class="language-plaintext highlighter-rouge">.kml</code> files into Earth Studio—though based on my testing, these files only support overlays and paths for visualization within the animation, rather than actual flight control.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/GoogleEarthStudio1-480.webp 480w,/assets/img/GoogleEarthStudio1-800.webp 800w,/assets/img/GoogleEarthStudio1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/GoogleEarthStudio1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Google Earth Studio templates" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">Google Earth Studio templates</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/GoogleEarthStudio2-480.webp 480w,/assets/img/GoogleEarthStudio2-800.webp 800w,/assets/img/GoogleEarthStudio2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/GoogleEarthStudio2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Google Earth Studio interface" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">Google Earth Studio main interface</p> <p>I exported the rendered animation as an <code class="language-plaintext highlighter-rouge">.mp4</code> file, which could be directly imported into both <strong>RealityCapture</strong> and <strong>PostShot</strong>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/AmesWindTunnelSpiralShort-480.webp 480w,/assets/img/AmesWindTunnelSpiralShort-800.webp 800w,/assets/img/AmesWindTunnelSpiralShort-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/AmesWindTunnelSpiralShort.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Google Earth Studio animation over NASA Ames wind tunnel. Attribution: Google Earth, Vexcel Imaging US, Inc." data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">Animation flyover of the NFAC at NASA Ames. Attribution: Google Earth, Vexcel Imaging US, Inc.</p> <hr/> <h4 id="step-2-realitycapture--convert-video-to-mesh">Step 2: RealityCapture – Convert Video to Mesh</h4> <p>In <strong>RealityCapture</strong>, I imported the <code class="language-plaintext highlighter-rouge">.mp4</code> and allowed it to extract approximately 200 frames. Using mostly default settings, I ran the photogrammetry pipeline to reconstruct a textured mesh.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/AmesWindTunnel-RealityCapture-480.webp 480w,/assets/img/AmesWindTunnel-RealityCapture-800.webp 800w,/assets/img/AmesWindTunnel-RealityCapture-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/AmesWindTunnel-RealityCapture.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Photogrammetry result in RealityCapture" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">Generated textured mesh from Google Earth Studio footage</p> <p>After reconstruction and simplification, I exported the mesh as a <code class="language-plaintext highlighter-rouge">.obj</code> file for further editing in Blender.</p> <hr/> <h4 id="step-3-blender--mesh-editing">Step 3: Blender – Mesh Editing</h4> <p>Photogrammetry-generated meshes often require cleanup. I used <strong>Blender</strong> to refine the geometry and make adjustments to prepare the model for 3D printing.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/AmesWindTunnel-Blender2-480.webp 480w,/assets/img/AmesWindTunnel-Blender2-800.webp 800w,/assets/img/AmesWindTunnel-Blender2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/AmesWindTunnel-Blender2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Editing the photogrammetry mesh in Blender" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">Post-processing and cleanup of the mesh in Blender</p> <hr/> <h4 id="step-4-orca-slicer--3d-printing">Step 4: Orca Slicer – 3D Printing</h4> <p>The refined mesh was then imported into <strong>Orca Slicer</strong>, where I set the model size and printing settings.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/AmesWindTunnel-Orca-480.webp 480w,/assets/img/AmesWindTunnel-Orca-800.webp 800w,/assets/img/AmesWindTunnel-Orca-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/AmesWindTunnel-Orca.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Preparing the mesh in Orca Slicer" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">Slicing the model in Orca Slicer</p> <hr/> <h4 id="step-5--print-results">Step 5 – Print Results</h4> <p>The final 3D print is shown below. The full model is roughly 12 cm across, with the wind tunnel itself clearly recognizable. Some areas could benefit from additional mesh refinement to ensure consistent flatness. Surrounding facilities could be better defined but are in most part still identifiable. One idea for future iterations would be multi-material printing to better distinguish between buildings, roads, and grass areas.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/AmesWindTunnel-3DPrint-480.webp 480w,/assets/img/AmesWindTunnel-3DPrint-800.webp 800w,/assets/img/AmesWindTunnel-3DPrint-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/AmesWindTunnel-3DPrint.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="3D Printed Model" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">Final result: 3D printed model of the NFAC</p> <hr/> <h4 id="extra-jawset-postshot--gaussian-splatting">Extra: Jawset PostShot – Gaussian Splatting</h4> <p>For comparison, I also processed the video using <strong>Jawset PostShot</strong> to create a <strong>Gaussian Splat</strong>. I used the <em>splat3 radiance field</em> profile and left most settings at default. The tool extracted around 150 images and reconstructed a lightweight splat-based rendering.</p> <p>One benefit of splatting is that it can represent background elements like the sky—something that traditional photogrammetry struggles with due to a lack of trackable features. However, because the animation included limited sky coverage, some regions remained unrepresented.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/AmesWindTunnel-Postshot-480.webp 480w,/assets/img/AmesWindTunnel-Postshot-800.webp 800w,/assets/img/AmesWindTunnel-Postshot-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/AmesWindTunnel-Postshot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Gaussian Splat rendering in PostShot" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">Gaussian Splat generated from the same animation using PostShot</p> <hr/> <h4 id="final-thoughts">Final Thoughts</h4> <p><strong>Google Earth Studio</strong> is a surprisingly capable tool for generating flyover-style animations, and it integrates well with both <strong>RealityCapture</strong> and <strong>Blender</strong> for experimentation and prototyping. I was able to produce a reasonably accurate physical model of the NFAC, entirely from virtual footage.</p> <p>As a future improvement, it would be useful if Earth Studio supported importing <code class="language-plaintext highlighter-rouge">.kml</code> flight plans for true photogrammetry simulation. Although <code class="language-plaintext highlighter-rouge">.kml</code> overlays are supported, they currently don’t drive the camera directly.</p> <p>Interestingly, the watermark embedded in the rendered video didn’t show up in either the mesh or splat results—likely rejected as noise by both systems.</p> <p>For those without access to drones or physical sites, I recommend exploring freely available photogrammetry datasets such as those hosted by <a href="https://www.esri.com/en-us/arcgis/products/arcgis-reality/resources/sample-drone-datasets">ESRI</a>. They’re a great resource for tuning your reconstruction pipeline before working with your own data.</p> <hr/> <p><strong>Tools Used:</strong></p> <ul> <li><a href="https://www.google.com/earth/studio/">Google Earth Studio</a></li> <li><a href="https://www.capturingreality.com/">RealityCapture (RealityScan)</a></li> <li><a href="https://www.jawset.com">Jawset PostShot</a></li> <li><a href="https://www.blender.org/">Blender</a></li> <li><a href="https://orca-slicer.com">Orca Slicer</a></li> </ul>]]></content><author><name></name></author><category term="graphics"/><category term="photogrammetry"/><category term="gaussian-splatting"/><category term="nasa"/><category term="blender"/><summary type="html"><![CDATA[Simulating drone imagery with Google Earth Studio and generating a 3D model with RealityCapture]]></summary></entry><entry><title type="html">Integrating Drones into Wildfire Fighting</title><link href="https://nlugon.github.io/blog/2025/Wildfiredrones/" rel="alternate" type="text/html" title="Integrating Drones into Wildfire Fighting"/><published>2025-06-20T10:00:00+00:00</published><updated>2025-06-20T10:00:00+00:00</updated><id>https://nlugon.github.io/blog/2025/Wildfiredrones</id><content type="html" xml:base="https://nlugon.github.io/blog/2025/Wildfiredrones/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nasa-acero1-480.webp 480w,/assets/img/nasa-acero1-800.webp 800w,/assets/img/nasa-acero1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/nasa-acero1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Wildfire Response with Drones" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">Drones offer new ways to monitor, track, and support wildfire response operations. Credits: NASA</p> <h3 id="1-introduction">1. Introduction</h3> <p>Wildfires are becoming <strong>more frequent, larger in scale, and more destructive</strong> than ever before. In 2024 alone, the U.S. saw nearly <strong>65,000 wildfires</strong> burn approximately <strong>8.9 million acres</strong>—one of the highest annual totals on record—and came just after the 2023 season, which burned 2.7 million acres. Meanwhile, wildfire suppression costs in 2023 reached <strong>$2.7 billion</strong>, contributing to over <strong>$3.17 billion in total damages</strong>. Beyond economic impacts, wildfires increasingly threaten lives, infrastructure, air quality, and can also exacerbate climate change through massive CO₂ emissions.</p> <h4 id="11-how-wildfires-work-the-fire-tetrahedron">1.1 How Wildfires Work: The Fire Tetrahedron</h4> <p>To understand how to fight wildfires, it helps to understand how they ignite and spread. The <strong>fire tetrahedron</strong> outlines the four key components required for fire:</p> <ul> <li><strong>Heat</strong></li> <li><strong>Fuel</strong></li> <li><strong>Oxygen</strong></li> <li><strong>Chemical chain reaction</strong></li> </ul> <p>Eliminating any one of these can suppress a fire. Traditional firefighting methods focus on removing fuel (e.g., controlled burns), reducing heat (e.g., blasting water), or preventing supply in oxygen and interfering with the chemical reaction (e.g., in part what fire retardants do).</p> <h4 id="12-state-of-the-art-in-wildfire-response">1.2 State of the Art in Wildfire Response</h4> <p>Current wildfire response involves a mix of satellite imaging, manned aircraft (like tankers and helicopters), ground crews, and predictive fire modeling. These methods offer valuable capabilities, but also face major <strong>limitations</strong>:</p> <ul> <li><strong>Surveillance is infrequent</strong> and often delayed</li> <li><strong>Aircraft operations are limited</strong> by daylight, visibility, and turbulence</li> <li><strong>Communications infrastructure is inconsistent</strong>, especially in remote areas</li> <li><strong>Multiple agencies</strong> must coordinate with different technologies</li> </ul> <p>NASA’s assessment with the U.S. Forest Service and other agencies confirmed these gaps. There’s a pressing need for <strong>persistent surveillance</strong>, <strong>interoperable communications</strong>, and <strong>modern airspace management</strong> to safely coordinate diverse aircraft, especially in chaotic emergency environments.</p> <h4 id="13-where-drones-fit-in">1.3 Where Drones Fit In</h4> <p>Drones can help <strong>fill critical gaps</strong> in wildfire response:</p> <ul> <li>They can fly <strong>day or night</strong>, even when visibility is poor</li> <li>They offer <strong>persistent aerial presence</strong>, acting as “extra eyes” for command centers</li> <li>They can be deployed rapidly for <strong>search and rescue</strong>, <strong>emergency deliveries</strong>, <strong>mapping</strong>, or <strong>live monitoring</strong></li> <li>They’re typically <strong>less costly and easier to deploy</strong> than manned aircraft</li> </ul> <p>Unlike manned aircraft, drones don’t depend on pilot availability or crew rest cycles. They can be automated, pre-positioned, and scaled across incidents. But to <strong>safely integrate</strong> drones into wildfire airspace—alongside helicopters, tankers, and manned spotter aircraft—we need new <strong>technological frameworks and tools</strong> for coordination.</p> <p>This is where NASA’s <strong>ACERO project</strong> steps in.</p> <hr/> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nasa-acero2-480.webp 480w,/assets/img/nasa-acero2-800.webp 800w,/assets/img/nasa-acero2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/nasa-acero2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="NASA ACERO" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">Advanced Capabilities for Emergency Response Operations (ACERO). Credits: NASA</p> <h3 id="2-introducing-acero-a-new-framework-for-aerial-wildfire-coordination">2. Introducing ACERO: A New Framework for Aerial Wildfire Coordination</h3> <p>To safely integrate drones into wildfire airspace, <strong>NASA Ames Research Center</strong> is leading the <strong>Advanced Capabilities for Emergency Response Operations (ACERO)</strong> project. ACERO addresses critical challenges in wildfire response by developing technologies for:</p> <ul> <li><strong>Wildfire airspace management</strong></li> <li><strong>Resilient communications and surveillance</strong></li> <li><strong>Coordinated operations of both manned and unmanned aircraft</strong></li> <li><strong>Human-centered decision support systems</strong></li> </ul> <p>One impactful outcome of ACERO so far is the development of the <strong>Portable Airspace Management System (PAMS)</strong>.</p> <h4 id="what-is-pams">What is PAMS?</h4> <p>PAMS is a <strong>rugged, field-deployable case</strong> that houses all the essential tools a drone operator or mission supervisor needs that allow to safely coordinate drone operations during wildfire or emergency response efforts. The case includes:</p> <ul> <li>A computer with display and mission coordination software</li> <li>Radios and networking gear for air-to-air and air-to-ground communication</li> <li>Support for <strong>real-time coordination between drones, helicopters, and command centers</strong></li> </ul> <p>Its purpose is to allow <strong>rapid, safe drone deployment in emergency scenarios</strong>, without compromising <strong>airspace safety</strong>. During a wildfire, decisions must be made in seconds—not minutes. PAMS enables teams to:</p> <ul> <li><strong>Quickly obtain airspace clearance</strong></li> <li><strong>Coordinate with manned aircraft crews</strong></li> <li><strong>Avoid airspace conflicts between drones and other emergency aircraft</strong></li> </ul> <p>By making situational awareness tools <strong>portable and self-contained</strong>, PAMS ensures that small teams operating in the field have access to the same coordination tools previously only available in large command centers.</p> <h4 id="designed-for-emergency-use">Designed for Emergency Use</h4> <p>In a wildfire, conditions evolve rapidly. PAMS was built with these challenges in mind:</p> <ul> <li><strong>Fast deployment and boot-up</strong></li> <li><strong>User-friendly interface</strong> so responders can focus on the mission, not the tech</li> <li><strong>Interoperable communication support</strong>, accounting for the fact that multiple agencies (local, state, federal) may use different radios or protocols</li> </ul> <p>In practice, PAMS functions like an <strong>on-the-go UAS traffic control center</strong>—helping drone pilots respond faster while maintaining visibility into a shared airspace environment. It’s not just about flying a drone—it’s about flying <strong>safely</strong>, in a space shared with crewed aircraft, multiple agencies, and unpredictable fire conditions.</p> <hr/> <h3 id="3-conclusion-demonstrations-human-factors-and-what-comes-next">3. Conclusion: Demonstrations, Human Factors, and What Comes Next</h3> <p>NASA’s ACERO team has already <strong>demonstrated early versions of this technology</strong> in the field—most notably in the rugged mountain ranges near <strong>Monterey, California</strong>. These flight exercises proved that drone coordination and wildfire airspace management can be executed more safely and efficiently using portable tools like PAMS.</p> <p>But this is just the beginning. Several key areas are now being improved and expanded:</p> <h4 id="human-centered-interface-design">Human-Centered Interface Design</h4> <p>When dealing with fast-changing emergencies, <strong>ease of use is essential</strong>. One major focus is refining the PAMS software interface to ensure:</p> <ul> <li><strong>Minimal training is needed</strong>, even for first-time users</li> <li>Interfaces are intuitive, clear, and reduce operator workload</li> <li>Critical information is displayed with appropriate prioritization and clarity</li> </ul> <p>The goal is to make the system so easy and reliable that firefighters and drone operators <strong>want to use it</strong>—even under stress.</p> <h4 id="flexible-communication-integration">Flexible Communication Integration</h4> <p>Wildfire response involves <strong>many agencies</strong>—fire departments, forest service, military, local law enforcement—each with their own tools and protocols. A current challenge is ensuring PAMS can <strong>integrate a wide variety of communication systems</strong>, including:</p> <ul> <li>VHF/UHF radios</li> <li>LTE/cellular and SATCOM</li> <li>Mesh networks and ground relays</li> </ul> <p>This level of flexibility is essential for <strong>interoperability in the field</strong> and is a key step toward making the technology widely adoptable.</p> <h4 id="towards-the-final-pams-case">Towards the Final PAMS Case</h4> <p>The ACERO team is also working on a finalized version of the PAMS unit—one that is:</p> <ul> <li><strong>Rugged</strong> enough for harsh environments</li> <li><strong>Compact and easy to deploy</strong></li> <li><strong>Modular</strong>, so it can adapt to future technologies or mission types</li> </ul> <p>Ultimately, the vision is to <strong>modernize aerial emergency response</strong> using scalable, interoperable, and field-proven tools—empowering drone teams to operate <strong>faster</strong>, <strong>safer</strong>, and <strong>more effectively</strong>, even in the most challenging conditions.</p> <h3 id="4-sidenote-drone-incursions-during-wildfires">4. Sidenote: Drone Incursions during Wildfires</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/wildfiredroneincursions-480.webp 480w,/assets/img/wildfiredroneincursions-800.webp 800w,/assets/img/wildfiredroneincursions-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/wildfiredroneincursions.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Drone Incursions During Wildfires (2025)" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted text-center mt-2">Sidenote-the problem of incursive drones. Credits: National Interagency Fire Center</p> <p>As of <strong>May 14, 2025</strong>, there have already been <strong>18 reported drone incursions</strong> during active wildfire incidents. Every single one of these incidents <strong>disrupted or delayed aerial firefighting operations</strong>, putting lives and property at risk.</p> <p>When an unidentified drone not involved in response efforts (i.e. there to capture dramatic footage) is spotted near a wildfire, <strong>firefighting aircraft must be grounded immediately</strong>. Pilots cannot risk a midair collision with an unknown UAV. These delays cost critical time, slow containment efforts, and can allow fires to grow unchecked.</p> <hr/> <h3 id="5-extra-portable-ground-radars-for-increased-awareness-and-safety">5. Extra: Portable Ground Radars for Increased Awareness and Safety</h3> <p>As the ACERO project progresses, one next step is the deployment of <strong>portable ground-based radar systems</strong> at the wildfire response perimeter. These systems would:</p> <ul> <li>Track and manage <strong>authorized drones and crewed aircraft</strong></li> <li>Detect and geolocate <strong>unauthorized or rogue drones</strong> in real-time</li> <li>Support <strong>safer and more scalable airspace operations</strong> in high-tempo emergencies</li> </ul> <p>This approach not only improves <strong>situational awareness</strong>, but also supports enforcement and safety—helping incident commanders take swift action if a drone threatens operations. Additional sidenote: foreshaddowing for a next blog post.</p> <h3 id="sources">Sources</h3> <ul> <li> <p><a href="https://www.nifc.gov/fire-information/statistics/wildfires">National Interagency Fire Center (NIFC) – Wildfire Statistics</a></p> </li> <li> <p><a href="https://www.nifc.gov/fire-information/statistics/suppression-costs">NIFC – Federal Firefighting Costs (Suppression Only)</a></p> </li> <li> <p><a href="https://report.firststreet.org/5th-National-Risk-Assessment-Fueling-the-Flames.pdf">First Street Foundation – 5th National Risk Assessment: Fueling the Flames</a></p> </li> <li> <p><a href="ttps://www.nasa.gov/directorates/armd/aosp/acero-wildfire/">NASA ACERO (Advanced Capabilities for Emergency Response Operations)</a></p> </li> <li> <p><a href="https://nari.arc.nasa.gov/events/tfrsac-wildfire">Tactical Fire Remote Sensing Advisory Committee (TFRSAC) Bi-Annual Meeting and Aeronautics Research Mission Directorate (ARMD) Wildfire Management Workshop</a> (+ <a href="https://nari.arc.nasa.gov/sites/default/files/attachments/Spring%202021%20TFRSAC%20Agenda%20with%20Presentation%20Links%20FINAL%2006012021.pdf">slides and videos</a>)</p> </li> <li> <p><a href="https://nari.arc.nasa.gov/sites/default/files/attachments/NASA%20ARMD%20WILDFIRE%20MANAGEMENT%20WORKSHOP_6.1.2021_v13.pdf">NASA Wildfire Management Workshop Report – Stakeholder Needs, Gaps, and Technology Opportunities</a></p> </li> <li> <p><a href="https://www.fs.usda.gov/managing-land/fire">US Forest Service – Fire Operations and Interagency Coordination Reports</a></p> </li> </ul> <hr/> <h3 id="image-credits">Image Credits</h3> <ul> <li><a href="https://www.nasa.gov/directorates/armd/aosp/acero-wildfire/">NASA</a></li> <li><a href="https://www.nifc.gov">NIFC</a></li> </ul>]]></content><author><name></name></author><category term="aerospace"/><category term="drones"/><category term="wildfire"/><category term="emergency-response"/><category term="acero"/><category term="nasa"/><summary type="html"><![CDATA[Exploring how drone technologies and NASA’s ACERO initiative can enhance wildfire response.]]></summary></entry><entry><title type="html">Space and Aerospace Websites to Check Out</title><link href="https://nlugon.github.io/blog/2025/cool-websites/" rel="alternate" type="text/html" title="Space and Aerospace Websites to Check Out"/><published>2025-01-25T18:00:00+00:00</published><updated>2025-01-25T18:00:00+00:00</updated><id>https://nlugon.github.io/blog/2025/cool-websites</id><content type="html" xml:base="https://nlugon.github.io/blog/2025/cool-websites/"><![CDATA[<p>Explore this selection of space and aerospace websites, offering tools, insights, and data to inspire and inform your journey through the cosmos.</p> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mars-trek-website-480.webp 480w,/assets/img/mars-trek-website-800.webp 800w,/assets/img/mars-trek-website-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mars-trek-website.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted mt-2" style="font-size: 0.9em;"> Image credit: <a href="https://trek.nasa.gov/mars/" target="_blank">NASA Mars Trek</a> </p> </div> <div class="col-sm mt-3 mt-md-0"> <h3><a href="https://trek.nasa.gov/mars/" target="_blank">NASA Mars Trek</a></h3> <p>An interactive mapping tool for Mars exploration, offering a detailed view of the Martian surface with data from multiple missions.</p> </div> </div> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dsn-website-480.webp 480w,/assets/img/dsn-website-800.webp 800w,/assets/img/dsn-website-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/dsn-website.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted mt-2" style="font-size: 0.9em;"> Image credit: <a href="https://eyes.nasa.gov/apps/dsn-now/" target="_blank">NASA DSN Now</a> </p> </div> <div class="col-sm mt-3 mt-md-0"> <h3><a href="https://eyes.nasa.gov/apps/dsn-now/" target="_blank">NASA DSN Now</a></h3> <p>Track real-time communication between spacecraft and NASA's Deep Space Network (DSN) antennas worldwide.</p> </div> </div> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nasa-airborne-science-480.webp 480w,/assets/img/nasa-airborne-science-800.webp 800w,/assets/img/nasa-airborne-science-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/nasa-airborne-science.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted mt-2" style="font-size: 0.9em;"> Image credit: <a href="https://airbornescience.nasa.gov/tracker/" target="_blank">NASA Airborne Science Tracker</a> </p> </div> <div class="col-sm mt-3 mt-md-0"> <h3><a href="https://airbornescience.nasa.gov/tracker/" target="_blank">NASA Airborne Science Tracker</a></h3> <p>Follow NASA's airborne science missions in real-time and learn about the aircraft and their research objectives.</p> </div> </div> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rocket-website-480.webp 480w,/assets/img/rocket-website-800.webp 800w,/assets/img/rocket-website-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rocket-website.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted mt-2" style="font-size: 0.9em;"> Image credit: <a href="https://rocketlaunch.org" target="_blank">Rocket Launch Schedule</a> </p> </div> <div class="col-sm mt-3 mt-md-0"> <h3><a href="https://rocketlaunch.org" target="_blank">Rocket Launch Schedule</a></h3> <p>Stay updated on upcoming rocket launches.</p> </div> </div> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/zoom-website-480.webp 480w,/assets/img/zoom-website-800.webp 800w,/assets/img/zoom-website-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/zoom-website.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted mt-2" style="font-size: 0.9em;"> Image credit: <a href="https://zoom.earth" target="_blank">Zoom Earth</a> </p> </div> <div class="col-sm mt-3 mt-md-0"> <h3><a href="https://zoom.earth" target="_blank">Zoom Earth</a></h3> <p>Offering near real-time satellite imagery, with a captivating view of global weather patterns, natural disasters, and other planetary phenomena.</p> </div> </div> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/moon-website-480.webp 480w,/assets/img/moon-website-800.webp 800w,/assets/img/moon-website-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/moon-website.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted mt-2" style="font-size: 0.9em;"> Image credit: <a href="https://quickmap.lroc.asu.edu/" target="_blank">Lunar QuickMap</a> </p> </div> <div class="col-sm mt-3 mt-md-0"> <h3><a href="https://quickmap.lroc.asu.edu/" target="_blank">Lunar QuickMap</a></h3> <p>A powerful tool for exploring the Moon’s surface, featuring data from NASA's Lunar Reconnaissance Orbiter.</p> </div> </div> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vfr-website-480.webp 480w,/assets/img/vfr-website-800.webp 800w,/assets/img/vfr-website-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/vfr-website.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted mt-2" style="font-size: 0.9em;"> Image credit: <a href="https://vfrmap.com" target="_blank">VFR Map</a> </p> </div> <div class="col-sm mt-3 mt-md-0"> <h3><a href="https://vfrmap.com" target="_blank">VFR Map</a></h3> <p>View detailed aeronautical maps for visual flight rules (VFR).</p> </div> </div> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/satellite-website-480.webp 480w,/assets/img/satellite-website-800.webp 800w,/assets/img/satellite-website-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/satellite-website.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted mt-2" style="font-size: 0.9em;"> Image credit: <a href="https://geoxc-apps.bd.esri.com/space/satellite-explorer/#" target="_blank">Esri Satellite Explorer</a> </p> </div> <div class="col-sm mt-3 mt-md-0"> <h3><a href="https://geoxc-apps.bd.esri.com/space/satellite-explorer/#" target="_blank">Esri Satellite Explorer</a></h3> <p>Visualize satellite orbits and track their movements in real-time.</p> </div> </div> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/satnogs-480.webp 480w,/assets/img/satnogs-800.webp 800w,/assets/img/satnogs-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/satnogs.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted mt-2" style="font-size: 0.9em;"> Image credit: <a href="https://db.satnogs.org" target="_blank">SatNOGS DB</a> </p> </div> <div class="col-sm mt-3 mt-md-0"> <h3><a href="https://db.satnogs.org" target="_blank">SatNOGS DB</a></h3> <p>A global open-source database of satellite transmissions collected by amateur ground stations—browse frequencies, decode signals, and explore satellite metadata.</p> </div> </div> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/leolabs-480.webp 480w,/assets/img/leolabs-800.webp 800w,/assets/img/leolabs-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/leolabs.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted mt-2" style="font-size: 0.9em;"> Image credit: <a href="https://platform.leolabs.space/visualization" target="_blank">LeoLabs Visualization Platform</a> </p> </div> <div class="col-sm mt-3 mt-md-0"> <h3><a href="https://platform.leolabs.space/visualization" target="_blank">LeoLabs Visualization Platform</a></h3> <p>Visualize satellites and space debris in low Earth orbit (LEO) in real-time—an essential tool for tracking and monitoring orbital traffic and collisions.</p> </div> </div> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/forest-watch2-480.webp 480w,/assets/img/forest-watch2-800.webp 800w,/assets/img/forest-watch2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/forest-watch2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="text-muted mt-2" style="font-size: 0.9em;"> Image credit: <a href="https://www.globalforestwatch.org/map/" target="_blank">Global Forest Watch</a> </p> </div> <div class="col-sm mt-3 mt-md-0"> <h3><a href="https://www.globalforestwatch.org/map/" target="_blank">Global Forest Watch</a></h3> <p>Visually engaging and interactive platform to monitor forest changes, deforestation, and fire alerts in real-time, for raising awareness about the impact of wildfires and environmental conservation efforts worldwide.</p> </div> </div>]]></content><author><name></name></author><category term="space-exploration"/><category term="space"/><category term="aerospace"/><category term="websites"/><summary type="html"><![CDATA[A collection of websites related to space and aerospace.]]></summary></entry><entry><title type="html">Moon and Mars Landings</title><link href="https://nlugon.github.io/blog/2025/geojson/" rel="alternate" type="text/html" title="Moon and Mars Landings"/><published>2025-01-05T12:06:00+00:00</published><updated>2025-01-05T12:06:00+00:00</updated><id>https://nlugon.github.io/blog/2025/geojson</id><content type="html" xml:base="https://nlugon.github.io/blog/2025/geojson/"><![CDATA[<p>Map showing the equivalent locations of Moon and Mars landings. Click on the points for more information.</p> <div id="map" style="height: 500px; width: 100%;"></div> <h2 id="sources">Sources</h2> <ol> <li><a href="https://science.nasa.gov/resource/apollo-landing-sites-with-moon-phases/">Apollo Landing Sites with Moon Phases - NASA</a></li> <li><a href="https://en.wikipedia.org/wiki/List_of_landings_on_extraterrestrial_bodies">List of landings on extraterrestrial bodies - Wikipedia</a></li> </ol>]]></content><author><name></name></author><category term="space-exploration"/><category term="planetary"/><category term="landings"/><category term="geojson"/><category term="maps"/><summary type="html"><![CDATA[Visualizing Moon and Mars Landing Locations.]]></summary></entry></feed>